---
title: "Insurance Analysis"
author: 
- Juan Manuel Jiménez
- Javier Pinilla
- Luis Rus
date: "06/05/2020"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: flatly
    code_folding: hide
---



# Objetivo del Proyecto

El objetivo del proyecto es poder identificar que factores de los asegurados afectan en la siniestralidad de las aseguradoras de autos. 

La siniestralidad afecta de lleno en la cuenta de resultados de la aseguradora y una buena comprensión de estos factores hará que la compañía pueda cuantificar bien el riesgo y por lo tanto el importe a cobrar al asegurado e incluso que pueda rechazar ciertos riesgos que no quiera asumir.

Nuestra variable objetivo será el indicador de si el cliente va a tener accidente o no, es decir, si el cliente es potencialmente buen cliente de la compañia o no lo es y en base a eso tarificaremos.

Nuestra medida a seguir es el numero máximo de asegurados bien categorizados (accuracy) dandole menos importancia a fallar en los que creemos que no van a tener accidente y luego tienen que al reves pero teniendo en mente el máximo acierto en los que van a tener accidente y lo tienen. 

Es decir el negocio asegurador se basa en volumenes pero estos deben ser con la mayor calidad posible. 
Si somos muy exigentes con la tarificacion de clientes nuevos (marcamos a muchos con el flag de va a tener accidente y no es asi) no seremos competitivos en el mercado por precio y no tendremos volumen de cartera para hacer frente a pagos. Al errar en los que van a tener accidente podemos ajustar nuestros modelos para las carteras en run-off en años posteriores teniendo más información de nuestros ya clientes pero para una contratación inicial no es aconsejable tener un modelo muy exijente.

# Técnicas y modelos utilizados.

En la practica hemos realizado varios experimentos para ver de que manera podiamos alcanzar el mejor accuracy para nuestra variable objetivo y para ello nos hemos valido de las siguientes técnicas en este orden.

1. Analisis de variables principales por medio de IV y WOE y de las variables mas importantes en arboles de decisión.

2. Utilización de modelos no supervisados para el analisis de la BBDD basados en clusterización con kmeans y hierarchical clustering

3. Técnicas de reducción de la dimensionalidad tales como PCA y TSNE.

4. Entrenamiento e hiperparametros de los siguientes modelos(paquete utilizado caret) en los cuales hacemos 9 lanzamientos para cada combinación de parametros:
        
   - Regresion Logistica: Analisis de mejores variables.
        
   - KNN: Parametro modificado el número de vecinos.
        
   - Decision Tree: modificando la complejidad del arbol.
        
   - Random forest: Cambiando el numero de variables en cada arbol, el tamaño minimo del nodo y  
     utilizando Gini para la pureza del nodo.
        
   - SVM: Modificando con relaciones entre sigma y C.
        
   - Naives Bayes.

5. Comparativa de modelos(train).
        
   - Comparativa de accuracy y kappa como media de los 9 lanzamientos de los mejores parametros de cada modelo.
        
   - Test de Wilcoxon para comprobar si podemos afirmar que las medias obtenidas son significativamente                      diferentes para cada modelo.

6. Comparativa de modelos (test)
        
   - Predicción dataset de test comparando accuracy y kappa.
        
   - Curva ROC

7. Eleccion del modelo final y lanzamiento de este con set de validación.

# Explicación breve de los distintos trabajos realizado.

Se han realizado 4 experimentos sobre nuestro dataset:

**1. División de la BBDD por clusterización (3 cluster) encontrando un cluster caracteristico y utilización de la otra parte de la BBDD por modelo. (opcion ganadora y presentada)**

 En esta opción tras realizar el WOE y quedarnos con las 3 variables mas explicativas aplicamos el kmeans viendo que hay  un cluster que representa el 20% de la bbdd de train que no tiene siniestralidad por lo que decidimos que este cluster  no lo incluimos para la predicción por modelos.

 Utilizamos WOE de nuevo desde las variables iniciales para el resto de BBDD que entrara en los modelos.

 Aplicamos cada uno de los modelos y finalmente decidimos aplicar el KNN a nuestro dataset de validación consiguiendo un  75% de accuracy de esta parte de la BBDD.

**2. División de la BBDD por clusterización (3 cluster) encontrando un cluster caracteristico y utilización de la otra parte de la BBDD dividida por los dos cluster que van por modelo**

 En esta opción como en la otra separamos un cluster quedando otros dos para los modelos pero en esta ocasion calculamos  las WOE de manera separada para cada uno y lo ejecutamos de manera separada.

 Para este caso vemos que en un cluster (34% de train) gana la opcion de GLM con un accuracy del 78% mientras que en el  otro cluster (24% de train) gana el knn con un 63% de accuracy.

**3. Obtención de las variables importantes por medio de IV y random forest, modificación de las variables por WOE y aplicación a los distintos modelos**

 En este caso nos quedabamos con 11 de las variables que aparecen como más importantes en IV cuyo indice está por encima  del 10% y tras realizar las modificaciones WOE aplicabamos al modelo obteniendo un accuracy del 78%.

**4. Obtención de las variables importantes por medio de IV y random forest y aplicación a los distintos modelos.**

 En este caso nos quedabamos con 11 de las variables que aparecen como más importantes en IV cuyo indice está por encima  del 10% y sin realizar las modificaciones WOE aplicabamos al modelo obteniendo un accuracy del 77%.

 Como comentario a destacar del trabajo decir que tras la realización del WOE con las tres variables más importantes y   aplicarlo al Kmeans los resultados son muy interesantes ya que los divide en 3 cluster con siniestralidad de 0%, 25%, y  50% que podría ser muy util para la tarificación.

# Trabajo desarrollado

Vamos a realizar la explicación detallada de la opción ganadora explicada brevemente arriba consistente en  la división de la BBDD por clusterización (3 cluster) encontrando un cluster caracteristico y utilización de la otra parte de la BBDD por modelo.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(egg)
library(GGally)
library(ISLR)
library(car)
library(DMwR2)
library(faraway)
library(mlbench)
library(knitr)
library(kableExtra)
library(htmltools)
library(bsplus)
library(RColorBrewer)
library(lattice)
library(VIM)
library(mice)
library(stringr)
library(corrplot)
library(tidyverse)
library(lsr)
library(cowplot)
library(tidymodels)
library(skimr)
library(tibble)
library(lubridate)
library(padr)
library(caret)
library(partykit)
library(DMwR)
library(lime)
library(parsnip)
library(randomForest)
library(cluster)
library(RColorBrewer)
library(factoextra)
library(readr)
library(dplyr)
library(stringr)
library(Rtsne)
library(pROC)
library(ggpubr)
library(ROCR)
library(rpart)
library(rpart.plot)
library(e1071)
library(leaps)
library(MASS)
library(Information)
library(discretization)
library(arules)
library(scorecard)
select <- dplyr::select
library(flexclust)

```

## Carga, Limpieza y transformaciones de los datos.

Contamos con un dataset que contiene informacion sobre el tomador y el vehículo a asegurar. La variable objetivo es en este caso CLAIM FLAG, nos indica si ha tenido accidente o no.


El dataset se compone de 10302 registros y 27 variables que vamos a explicar a continuación:

**ID** - ID de poliza

**KIDSDRIV** - Numero de adolescentes con licencia de conducir en la unidad familiar.

**BIRTH** -	Año de nacimiento.

**AGE** -	Edad.

**HOMEKIDS** - Adolescentes viviendo en la casa de la unidad familiar.

**YOJ** - Años en el trabajo.

**INCOME** - Ingresos anuales.

**PARENT1** - Flag de Familia uniparental.

**HOME_VAL**- Valor de la vivienda.

**MSTATUS**- Estado Civil (Soltero-Casado).

**GENDER**- Sexo.

**EDUCATION**- Nivel de educación.

**OCCUPATION**-	Ocupacion.

**TRAVTIME**- Tiempo diario en minutos al centro de trabajo.

**CAR_USE**- Uso del vehiculo.

**BLUEBOOK**- Valor actual del vehiculo.

**TIF**- Tiempo en vigor en la compañía actual.

**CAR_TYPE**- Tipo de vehiculo.

**RED_CAR**- Flag Vehiculo de color Rojo.

**OLDCLAIM**- Valor de los siniestros anteriores al año en curso.

**CLM_FREQ**- Frecuencia de los siniestros anteriores al año en curso.

**REVOKED**- Siniestro revocado por la compañia.

**MVR_PTS**- Nº de puntos retirados del carnet.

**CLM_AMT**- Cuantia del siniestro en el año en curso.

**CAR_AGE**- Años del vehiculo.

**CLAIM_FLAG**- Flag de siniestro año en curso.

**URBANICITY**- Lugar de conduccion - Urbano vs. Rural

Vamos a dividir los registros en 3 subsets llamados:

train: Se compone de 7004 registros (70%)

test: Se compone de 2061 registros (20%)

validación: Se compone de 1237 (10%)

```{r, echo=FALSE}
insurance <- read.csv(file="car_insurance_claim.csv", header=TRUE, sep=",")
dim(insurance)
```

```{r, echo=FALSE}
set.seed(101)
sample <- sample.int(n = nrow(insurance), size = floor(.80*nrow(insurance)), replace = F)
train_temp <- insurance[sample, ]
test  <- insurance[-sample, ]
# Validation
sample_validation <- sample.int(n = nrow(train_temp), size = floor(.85*nrow(train_temp)), replace = F)
train <- train_temp[sample_validation,]
validation <- train_temp[-sample_validation,]

# Contamos el numero de líneas para el análisis.
n_train <- nrow(train)
n_test <- nrow(test)
n_validation <- nrow(validation)
n_train
n_test
n_validation
```

Tras realizar las transformaciones pertinentes de formatos e imputación de valores faltantes y analizar las variables y eliminar las que no aportan valor como el Id de cliente u otras muy relacionadas con la variable objetivo como el valor del siniestro(si hay siniestro hay valor) ya explicadas en la práctica anterior el dataset se queda de la siguiente manera:

```{r, echo=FALSE}
# Cambiar "," por "." y eliminar los $
train$INCOME <- str_replace_all(train$INCOME, ",", "")
train$INCOME <- str_sub(train$INCOME, 2, -1)
train$HOME_VAL <- str_replace_all(train$HOME_VAL, ",", "")
train$HOME_VAL <- str_sub(train$HOME_VAL, 2, -1)
train$BLUEBOOK <- str_replace_all(train$BLUEBOOK, ",", "")
train$BLUEBOOK <- str_sub(train$BLUEBOOK, 2, -1)
train$OLDCLAIM <- str_replace_all(train$OLDCLAIM, ",", "")
train$OLDCLAIM <- str_sub(train$OLDCLAIM, 2, -1)
train$CLM_AMT <- str_replace_all(train$CLM_AMT, ",", "")
train$CLM_AMT <- str_sub(train$CLM_AMT, 2, -1)
# Cambiar variables tipo "character" a "numeric"
train$INCOME <- as.numeric(train$INCOME)
train$HOME_VAL <- as.numeric(train$HOME_VAL)
train$BLUEBOOK <- as.numeric(train$BLUEBOOK)
train$OLDCLAIM <- as.numeric(train$OLDCLAIM)
train$CLM_AMT <- as.numeric(train$CLM_AMT)
```


```{r, echo=FALSE, warning=FALSE}
# Quitar prefijo z_
train <- train %>% mutate_if(is.factor,funs(str_replace(.,"z_", "")))

train$OCCUPATION <- factor(train$OCCUPATION,
                          ordered = FALSE,
                          levels = c("", "Clerical", "Doctor", "Home Maker", "Lawyer", "Manager", "Professional", "Student", "Blue Collar"),
                          labels = c("Others", "Clerical", "Doctor", "Home Maker", "Lawyer", "Manager", "Professional", "Student", "Blue Collar"))

# Pasar columnas tipo "character" a "factor"
train <- train %>% mutate_if(is.character, as.factor)
# Creamos una tabla con las variables y los valores que transformaremos con el valor 1
variables_binarias <- data.frame(Variable = c("CAR_USE","MSTATUS","PARENT1","RED_CAR","REVOKED","GENDER","URBANICITY"),
	valor_1 = c("Commercial","Yes","Yes","yes","Yes","M","Highly Urban/ Urban"),
	stringsAsFactors=FALSE)
for(i in 1:length(variables_binarias$Variable)){
  var <- variables_binarias$Variable[i]
  true_value <- variables_binarias$valor_1[i]
  train[,var] <- ifelse(train[,var] == true_value,1,0)
}
```


```{r, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
# INCOME
stoch_imp <- train %>% VIM::kNN(., variable=c("INCOME","BLUEBOOK"))
d1 = train$INCOME
train$INCOME <- stoch_imp$INCOME
x <- data.frame(d1 = d1, d2 = stoch_imp$INCOME)
data <- melt(x)
#ggplot(data,aes(x=value, fill=variable)) + geom_density(alpha=0.25) + labs(title = "INCOME")

# CAR_AGE
stoch_imp <- train %>% VIM::kNN(., variable=c("CAR_AGE", "INCOME","EDUCATION"))
d1 = train$CAR_AGE
train$CAR_AGE <- stoch_imp$CAR_AGE
x <- data.frame(d1 = d1, d2 = stoch_imp$CAR_AGE)
data <- melt(x)
#ggplot(data,aes(x=value, fill=variable)) + geom_density(alpha=0.25) + labs(title = "CAR AGE")
train <- train[train$CAR_AGE > -1,]

# YOJ
stoch_imp <- train %>% VIM::kNN(., variable=c("INCOME","YOJ","EDUCATION", "HOMEKIDS", "AGE"))
d1 = train$YOJ
train$YOJ <- stoch_imp$YOJ
x <- data.frame(d1 = d1, d2 = stoch_imp$YOJ)
data <- melt(x)
#ggplot(data,aes(x=value, fill=variable)) + geom_density(alpha=0.25) + labs(title = "YOJ")

# AGE
train$BIRTH <- as.character(train$BIRTH)
train <- train %>% mutate(AGE=ifelse(is.na(AGE), 120-as.numeric(substr(BIRTH, nchar(BIRTH)-1, nchar(BIRTH))), AGE))

# HOME_VAL
stoch_imp <- train %>%  VIM::kNN(., variable=c("INCOME","HOME_VAL"), k=1)
train$INCOME <- stoch_imp$INCOME
train$HOME_VAL <- stoch_imp$HOME_VAL
```
```{r echo=FALSE}
train$EDUCATION <- factor(train$EDUCATION,
                          ordered = TRUE,
                          levels = c("<High School", "High School", "Bachelors", "Masters", "PhD"),
                          labels = c("1-<High School", "2-High School", "3-Bachelors", "4-Masters", "PhD"))
```

```{r echo=FALSE}
train$bin_OLDCLAIM <- ifelse(train$OLDCLAIM > 0,1,0)
train$bin_HOMEKIDS <- ifelse(train$HOMEKIDS > 3,1,0)
train["bin_KIDSDRIV"] <- ifelse(train["KIDSDRIV"] > 0,1,0)
train["bin_TIF"] <- ifelse(train["TIF"] < 3 | train["TIF"] > 15 ,0,1)
train["bin_AGE"] <- ifelse(train["AGE"] < 26 | train["AGE"] > 57 ,1,0)
train["bin_MVR_PTS"] <- ifelse(train["MVR_PTS"] > 6,1,0)
train["bin_OCCUPATION"] <- ifelse(train["OCCUPATION"] == "Lawyer" | train["OCCUPATION"] == "Manager"| train["OCCUPATION"] == "Doctor" ,0,1)
train["bin_EDUCATION"] <- ifelse(train["EDUCATION"] == "1-<High School" | train["EDUCATION"] == "2-High School",1,0)
train["bin_CAR_TYPE"] <- ifelse(train["CAR_TYPE"] == "Minivan",0,1)
# Variables logarítmicas
train$CLM_AMT_log <- log(train$CLM_AMT,10)
train$INCOME_log <- log(train$INCOME+1,10)
train$BLUEBOOK_log <- log(train$BLUEBOOK,10)
train$OLDCLAIM_log <- log(train$OLDCLAIM+1,10)
```


```{r echo=FALSE}
# Cambiar "," por "." y eliminar los $
test$INCOME <- str_replace_all(test$INCOME, ",", "")
test$INCOME <- str_sub(test$INCOME, 2, -1)
test$HOME_VAL <- str_replace_all(test$HOME_VAL, ",", "")
test$HOME_VAL <- str_sub(test$HOME_VAL, 2, -1)
test$BLUEBOOK <- str_replace_all(test$BLUEBOOK, ",", "")
test$BLUEBOOK <- str_sub(test$BLUEBOOK, 2, -1)
test$OLDCLAIM <- str_replace_all(test$OLDCLAIM, ",", "")
test$OLDCLAIM <- str_sub(test$OLDCLAIM, 2, -1)
test$CLM_AMT <- str_replace_all(test$CLM_AMT, ",", "")
test$CLM_AMT <- str_sub(test$CLM_AMT, 2, -1)
# Cambiar variables tipo "character" a "numeric"
test$INCOME <- as.numeric(test$INCOME)
test$HOME_VAL <- as.numeric(test$HOME_VAL)
test$BLUEBOOK <- as.numeric(test$BLUEBOOK)
test$OLDCLAIM <- as.numeric(test$OLDCLAIM)
test$CLM_AMT <- as.numeric(test$CLM_AMT)
# Quitar prefijo z_
test <- test %>% mutate_if(is.factor,funs(str_replace(.,"z_", "")))
# Renombrado labels variable OCCUPATION
test$OCCUPATION <- factor(test$OCCUPATION,
                          ordered = FALSE,
                          levels = c("", "Clerical", "Doctor", "Home Maker", "Lawyer", "Manager", "Professional", "Student", "Blue Collar"),
                          labels = c("Others", "Clerical", "Doctor", "Home Maker", "Lawyer", "Manager", "Professional", "Student", "Blue Collar"))
test$EDUCATION <- factor(test$EDUCATION,
                          ordered = TRUE,
                          levels = c("<High School", "High School", "Bachelors", "Masters", "PhD"),
                          labels = c("1-<High School", "2-High School", "3-Bachelors", "4-Masters", "PhD"))
```



```{r echo=FALSE}
# Pasar columnas tipo "character" a "factor"
test <- test %>% mutate_if(is.character, as.factor)
# Creamos una tabla con las variables y los valores que transformaremos con el valor 1
for(i in 1:length(variables_binarias$Variable)){
  var <- variables_binarias$Variable[i]
  true_value <- variables_binarias$valor_1[i]
  test[,var] <- ifelse(test[,var] == true_value,1,0)
}
# Imputación de datos faltantes
train_knn_imp <- train %>% select(INCOME, BLUEBOOK, CAR_AGE, EDUCATION, YOJ, HOMEKIDS, AGE)
test_knn_imp <- test %>% select(INCOME, BLUEBOOK, CAR_AGE, EDUCATION, YOJ, HOMEKIDS, AGE)
train_knn_imp$isTrain <- rep(1, nrow(train_knn_imp))
test_knn_imp$isTrain <- rep(0, nrow(test_knn_imp))
train_test_knn_imp <- rbind(train_knn_imp,test_knn_imp)
```

```{r echo=FALSE}
# Imputado de missings varible INCOME
knn_income_imp <- train_test_knn_imp %>% VIM::kNN(., variable=c("INCOME","BLUEBOOK")) %>% filter(isTrain==0)
test$INCOME <- knn_income_imp$INCOME
# Imputado de missings varible CAR_AGE
knn_carage_imp <- train_test_knn_imp %>% VIM::kNN(., variable=c("CAR_AGE", "INCOME","EDUCATION")) %>%   filter(isTrain==0)
test$CAR_AGE <- knn_carage_imp$CAR_AGE
# Imputado de missings varible YOJ
knn_yoj_imp <- train_test_knn_imp %>% VIM::kNN(., variable=c("INCOME","YOJ","EDUCATION", "HOMEKIDS", "AGE")) %>% filter(isTrain==0)
test$YOJ <- knn_yoj_imp$YOJ
# Imputado de missings variable AGE
test$BIRTH <- as.character(test$BIRTH)
test <- test %>% mutate(AGE=ifelse(is.na(AGE), 120-as.numeric(substr(BIRTH, nchar(BIRTH)-1, nchar(BIRTH))), AGE))

test$bin_OLDCLAIM <- ifelse(test$OLDCLAIM > 0,1,0)
test$bin_HOMEKIDS <- ifelse(test$HOMEKIDS > 3,1,0)
test["bin_KIDSDRIV"] <- ifelse(test["KIDSDRIV"] > 0,1,0)
test["bin_TIF"] <- ifelse(test["TIF"] < 3 | test["TIF"] > 15 ,0,1)
test["bin_AGE"] <- ifelse(test["AGE"] < 26 | test["AGE"] > 57 ,1,0)
test["bin_MVR_PTS"] <- ifelse(test["MVR_PTS"] > 6,1,0)
test["bin_OCCUPATION"] <- ifelse(test["OCCUPATION"] == "Lawyer" | test["OCCUPATION"] == "Manager"| test["OCCUPATION"] == "Doctor" ,0,1)
test["bin_EDUCATION"] <- ifelse(test["EDUCATION"] == "1-<High School" | test["EDUCATION"] == "2-High School",1,0)
test["bin_CAR_TYPE"] <- ifelse(test["CAR_TYPE"] == "Minivan",0,1)
# Variables logarítmicas
test$CLM_AMT_log <- log(test$CLM_AMT,10)
test$INCOME_log <- log(test$INCOME+1,10)
test$BLUEBOOK_log <- log(test$BLUEBOOK,10)
test$OLDCLAIM_log <- log(test$OLDCLAIM+1,10)
```


```{r results = "hide"}
# MODIFICACIONES PARA EL SUBSET DE VALIDATION
# Cambiar "," por "." y eliminar los $
validation$INCOME <- str_replace_all(validation$INCOME, ",", "")
validation$INCOME <- str_sub(validation$INCOME, 2, -1)

validation$HOME_VAL <- str_replace_all(validation$HOME_VAL, ",", "")
validation$HOME_VAL <- str_sub(validation$HOME_VAL, 2, -1)

validation$BLUEBOOK <- str_replace_all(validation$BLUEBOOK, ",", "")
validation$BLUEBOOK <- str_sub(validation$BLUEBOOK, 2, -1)

validation$OLDCLAIM <- str_replace_all(validation$OLDCLAIM, ",", "")
validation$OLDCLAIM <- str_sub(validation$OLDCLAIM, 2, -1)

validation$CLM_AMT <- str_replace_all(validation$CLM_AMT, ",", "")
validation$CLM_AMT <- str_sub(validation$CLM_AMT, 2, -1)

# Cambiar variables tipo "character" a "numeric"
validation$INCOME <- as.numeric(validation$INCOME)

validation$HOME_VAL <- as.numeric(validation$HOME_VAL)

validation$BLUEBOOK <- as.numeric(validation$BLUEBOOK)

validation$OLDCLAIM <- as.numeric(validation$OLDCLAIM)

validation$CLM_AMT <- as.numeric(validation$CLM_AMT)

# Quitar prefijo z_
validation <- validation %>% mutate_if(is.factor,funs(str_replace(.,"z_", "")))

# Renombrado labels variable OCCUPATION
validation$OCCUPATION <- factor(validation$OCCUPATION,
                          ordered = FALSE,
                          levels = c("", "Clerical", "Doctor", "Home Maker", "Lawyer", "Manager", "Professional", "Student", "Blue Collar"),
                          labels = c("Others", "Clerical", "Doctor", "Home Maker", "Lawyer", "Manager", "Professional", "Student", "Blue Collar"))
validation$EDUCATION <- factor(validation$EDUCATION,
                          ordered = TRUE,
                          levels = c("<High School", "High School", "Bachelors", "Masters", "PhD"),
                          labels = c("1-<High School", "2-High School", "3-Bachelors", "4-Masters", "PhD"))

# Pasar columnas tipo "character" a "factor"
validation <- validation %>% mutate_if(is.character, as.factor)

# Creamos una tabla con las variables y los valores que transformaremos con el valor 1
for(i in 1:length(variables_binarias$Variable)){
  var <- variables_binarias$Variable[i]
  true_value <- variables_binarias$valor_1[i]
  validation[,var] <- ifelse(validation[,var] == true_value,1,0)
}

# Imputación de datos faltantes

train_knn_imp <- train %>% select(INCOME, BLUEBOOK, CAR_AGE, EDUCATION, YOJ, HOMEKIDS, AGE)
validation_knn_imp <- validation %>% select(INCOME, BLUEBOOK, CAR_AGE, EDUCATION, YOJ, HOMEKIDS, AGE)

train_knn_imp$isTrain <- rep(1, nrow(train_knn_imp))
validation_knn_imp$isTrain <- rep(0, nrow(validation_knn_imp))
train_validation_knn_imp <- rbind(train_knn_imp,validation_knn_imp)
sapply(validation, function(x) sum(is.na(x)))

# Imputado de missings varible INCOME
knn_income_imp <- train_validation_knn_imp %>% VIM::kNN(., variable=c("INCOME","BLUEBOOK")) %>% filter(isTrain==0)
validation$INCOME <- knn_income_imp$INCOME

# Imputado de missings varible CAR_AGE
knn_carage_imp <- train_validation_knn_imp %>% VIM::kNN(., variable=c("CAR_AGE", "INCOME","EDUCATION")) %>%   filter(isTrain==0)
validation$CAR_AGE <- knn_carage_imp$CAR_AGE

# Imputado de missings varible YOJ
knn_yoj_imp <- train_validation_knn_imp %>% VIM::kNN(., variable=c("INCOME","YOJ","EDUCATION", "HOMEKIDS", "AGE")) %>% filter(isTrain==0)
validation$YOJ <- knn_yoj_imp$YOJ

# Imputado de missings variable AGE
validation$BIRTH <- as.character(validation$BIRTH)
validation <- validation %>% mutate(AGE=ifelse(is.na(AGE), 120-as.numeric(substr(BIRTH, nchar(BIRTH)-1, nchar(BIRTH))), AGE))
#sapply(validation, function(x) sum(is.na(x)))

validation["education_bin"] <- ifelse(validation["EDUCATION"] == "1-<High School" | validation["EDUCATION"] == "2-High School",1,0)
validation_target <- validation
validation_target$CLM_AMT_log <- log(validation$CLM_AMT,10)
validation_target$BLUEBOOK_log <- log(validation$BLUEBOOK,10)
validation_target$OLDCLAIM_log <- log(validation$OLDCLAIM+1,10)
```



```{r echo=FALSE}

#BBDD para analisis

df_train <- train %>% select("OLDCLAIM", "HOMEKIDS", "KIDSDRIV","TIF","AGE","MVR_PTS",
"OCCUPATION","EDUCATION","CAR_TYPE","CLAIM_FLAG", "YOJ","PARENT1","GENDER", "TRAVTIME","CAR_USE","RED_CAR","CLM_FREQ", "URBANICITY", "BLUEBOOK","MSTATUS")

str(df_train)
```

De igual manera se realizan las modificaciones para los sets de Test y Validación.

```{r echo=FALSE}

df_test <- test %>% select("OLDCLAIM", "HOMEKIDS", "KIDSDRIV","TIF","AGE","MVR_PTS","OCCUPATION","EDUCATION","CAR_TYPE","CLAIM_FLAG", "YOJ","PARENT1","GENDER", "TRAVTIME","CAR_USE","RED_CAR","CLM_FREQ", "URBANICITY", "BLUEBOOK","MSTATUS")

#test_kmeans <- df_test %>% mutate_if(is.factor,as.numeric) %>% mutate_if(is.integer,as.numeric)

#test_WOE <- test %>% select("OLDCLAIM","AGE","MVR_PTS","OCCUPATION","EDUCATION","CAR_TYPE","CLAIM_FLAG","CLM_FREQ", "URBANICITY", "BLUEBOOK","PARENT1", "CAR_USE")

#df_train$CLAIM_FLAG <- factor(df_train$CLAIM_FLAG, levels = c("0", "1"), labels = c("NOacc", "SIacc"))
```

```{r echo=FALSE}

df_validation <- validation %>% select("OLDCLAIM", "HOMEKIDS", "KIDSDRIV","TIF","AGE","MVR_PTS","OCCUPATION","EDUCATION","CAR_TYPE","CLAIM_FLAG", "YOJ","PARENT1","GENDER", "TRAVTIME","CAR_USE","RED_CAR","CLM_FREQ", "URBANICITY", "BLUEBOOK","MSTATUS")

#validation_kmeans <- df_validation %>% mutate_if(is.factor,as.numeric) %>% mutate_if(is.integer,as.numeric)

#validation_WOE <- validation %>% select("OLDCLAIM","AGE","MVR_PTS","OCCUPATION","EDUCATION","CAR_TYPE","CLAIM_FLAG","CLM_FREQ", "URBANICITY", "BLUEBOOK","PARENT1", "CAR_USE")


#df_train$CLAIM_FLAG <- factor(df_train$CLAIM_FLAG, levels = c("0", "1"), labels = c("NOacc", "SIacc"))
```

## Analisis de variables relevantes


Primero vamos a hacer un analisis de variables relevantes para la predicción de nuestra variable objetivo.

Para ello vamos a utilizar tanto la tecnica de information value como la ordenación de variables importantes del random forest.

Nos sacamos el listado de variables con el information value asociado viendo que hay tres variables que destacan del resto


```{r echo=FALSE, message = FALSE}

iv = iv(df_train, y = 'CLAIM_FLAG') %>%
  as_tibble() %>%
  mutate( info_value = round(info_value, 3) ) %>%
  arrange( desc(info_value) ) 

iv %>%
  knitr::kable()
```

Hemos analizado la variable por encima de 0.5 y tiene sentido que sea asi ya que son los siniestros pasados que sabemos que para las compañías de seguros es un factor muy predictivo.

Además hemos analizado que divisiones hace con las tres primeras variables en importancia.

```{r echo=FALSE, message = FALSE}

iv = iv(df_train, y = 'CLAIM_FLAG') %>%
  as_tibble() %>%
  mutate( info_value = round(info_value, 3) ) %>%
  arrange( desc(info_value) )%>% 
  filter(info_value >= 0.4)%>% 
  filter(info_value < 0.55)

iv %>%
  knitr::kable()
```

Tambien hemos tomado la importancia de las variables en un modelo de Random forest para comparar con la que nos sale del modelo de IV y vemos que las variables que están en primeros puesto de importancia coinciden en ambos modelos por lo que podemos asegurar que estas variables son importantes para el modelo.

```{r echo=FALSE, message = FALSE}
bins <- woebin(df_train %>% select(iv$variable, CLAIM_FLAG), y = "CLAIM_FLAG",  positive = "1")
```

```{r echo=FALSE}
bins$OLDCLAIM %>%
  knitr::kable()

bins$BLUEBOOK %>%
  knitr::kable()

bins$URBANICITY %>%
  knitr::kable()
```

```{r cache=TRUE, warning=FALSE}

#pred_prob_rf_2 <- predict(modelo_rf, test_random_forest, type = "prob")
#head(pred_prob_rf_2)
#head(modelo_rf)
set.seed(342)
z_train=sample(1:nrow(df_train),5500)
model.rf=randomForest(df_train$CLAIM_FLAG ~ . , data = df_train, subset = z_train, importance = TRUE, proximity = TRUE)
#model.rf

#head(train.rf)
#plot(model.rf)
varImpPlot(model.rf)
```

Como podemos ver si comparamos ambos modelos de importancia las variables principales aunque en distinto orden coinciden en la parte alta con lo que podemos asumir que la elección de variables por importancia es adecuada.

Finalmente nos vamos a quedar con aquellas cuyo factor explicativo es mayor que 0.4 en el indice de IV para aplicarlo en los modelos no supervisados de clustering.


```{r echo=FALSE, message = FALSE}
#df_train_woe <- woebin_ply(df_train %>% select(iv$variable, CLAIM_FLAG, TIF, "HOMEKIDS", #"KIDSDRIV","TIF","MVR_PTS", "YOJ","PARENT1","GENDER", "TRAVTIME","CAR_USE","RED_CAR"), bins = bins)

df_train_woe <- woebin_ply(df_train %>% select(iv$variable, CLAIM_FLAG), bins = bins)
df_test_woe <- woebin_ply(df_test %>% select(iv$variable, CLAIM_FLAG), bins = bins)
df_validation_woe <- woebin_ply(df_validation %>% select(iv$variable, CLAIM_FLAG), bins = bins)

df_train_woe <- df_train_woe
train_kmeans <- df_train_woe
df_test_woe <- df_test_woe
test_kmeans <- df_test_woe
```



## Modelo no supervisado

Con los modelos no supervisados vamos a intentar clasificar por grupos los registros para cuando lleguen nuevos registros poder imputarlos a un grupo de los seleccionados.

Con este metodo podemos ver si en los distintos cluster tenemos distintas probabilidades de siniestro y de esa manera podríamos tener distintas tarifas dependiendo del grupo en el que cayeran los nuevos registros.

### Modelo no supervisado KMEANS

Este modelo esta basado en la minimización de la suma de distancias entre cada objeto y el centroide de su grupo o cluster.

Al ser un modelo de distancias es necesario escalar todas las variables. Para ello utilizamos la función Scale.

Para nuestro modelo unicamente van a entrar las 3 variables que hemos seleccionado con el WOE


```{r echo=FALSE}
xiris <- scale(select(df_train_woe, - CLAIM_FLAG))
```

Primero vamos a buscar el punto optimo de cluster en funcion del minimo error.
 
Como vemos en la figura la pendiente hasta el cluster 3 es muy vertical empezando a decaer en el 4 por lo que elegiremos 3 como numero de centroides. 
 
```{r echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Initialize total within sum of squares error: wss
wss <- 0
wsr <- 0
set.seed(342)
# For 1 to 15 cluster centers
for (i in 1:10) {
  km.out <- kmeans(xiris, centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
  wsr[i] <- km.out$betweenss
}

# Plot total within sum of squares vs. number of clusters
plot(1:10, wss, type = "b", 
     xlab = "Número de Clusters", 
     ylab = "Suma de cuadrados DENTRO DE grupos")

plot(1:10, wsr, type = "b", 
     xlab = "Número de Clusters", 
     ylab = "suma Cuadrados ENTRE grupos")

```

Posteriormente al elegir el numero de centroides vamos a ejecutar el modelo y ver las medias de cada variable en cada Cluster y de este modo poder ver si hay diferencias en la siniestralidad (Variable CLAIM FLAG)

Lo que podemos ver que en los diferentes cluster es que tenemos tres porcentajes de siniestralidad diferenciados siendo uno de ellos muy llamativo ya que es practicamente 0 . Además los otros dos están claramente diferenciados siendo la siniestralidad de 25% y de 40% respectivamente.

Nos encontramos con un cluster que tiene un 20% de la cartera de train sin siniestralidad y otro que representa un 43% de la cartera con un 25%.

Finalmente lanzamos el set de Test para comprobar que los cluster se mantienen en porcentajes de siniestralidad.

Para ello hemos aplicado la funcion predict.kmeans de la pagina de R la cual asigna la misma asignación de clusterización. Previamente tambien habiamos aplicado a test el mismo WOE que obteniamos en train con la función woebin_ply.

```{r echo=FALSE, cache=TRUE}
set.seed(342)
km <- kmeans(xiris, 3, nstart = 1)
aggregate(df_train_woe, by=list(km$cluster), mean)


#plot(train_kmeans$AGE, train_kmeans$CLAIM_FLAG, col=km$cluster, xlab="age", ylab="claim_flag")
table(km$cluster, df_train_woe$CLAIM_FLAG)

xiris.test <- scale(select(df_test_woe, - CLAIM_FLAG))
xiris.validation <- scale(select(df_validation_woe, - CLAIM_FLAG))

predict.kmeans <- function(object, newdata){
    centers <- object$centers
    n_centers <- nrow(centers)
    dist_mat <- as.matrix(dist(rbind(centers, newdata)))
    dist_mat <- dist_mat[-seq(n_centers), seq(n_centers)]
    max.col(-dist_mat)
}

km.test <- predict(km, xiris.test)
km.validation <- predict(km, xiris.validation)

table_train <- prop.table(table(km$cluster, df_train_woe$CLAIM_FLAG), margin = 1) %>% round(digits=2)
colnames(table_train) <- c("0.train", "1.train")
table_train

table_test <- prop.table(table(km.test, df_test_woe$CLAIM_FLAG), margin = 1) %>% round(digits=2)
colnames(table_test) <- c("0.test", "1.test")
table_test
```

Tras los resultados en test muy en linea con el de train podemos dar por buena la clusterización y tomaremos acciones a raiz de estos resultados para nuestro estudio.

### Modelo no supervisado HIERARCHICAL CLUSTERING.

Con el modelo de Hierarchical clustering esperariamos un resultado similar al anterior ya que ambos modelos miran similaridades entre los datos y ambos usan la misma aproximación entre clusters. 

Graficamente podemos ver los 3 grupos claramente diferenciados.

```{r echo=FALSE, cache=TRUE}
clusters <- hclust(dist(select(df_train_woe, - CLAIM_FLAG)))
plot(clusters)
```


```{r echo=FALSE, cache=TRUE}
clusterCut <- cutree(clusters, 3)
#prop.table(table(clusterCut, df_train_woe$CLAIM_FLAG)) %>% round(digits=2)

```
 
 
```{r echo=FALSE, cache=TRUE}

#GENERAMOS LAS BBDD CON EL CAMPO CLUSTER INCLUIDO Y SIN LAS 3 VARIABLES QUE LO CLUSTERIZARON. ADEMAS FILTRAMOS EL CLUSTER 1 QUE ES EL DE SINIESTRALIDAD CASI CERO

df_train_clus <- df_train
df_train_clus$cluster <- km$cluster
df_train_iv <- select(df_train_clus,- "OLDCLAIM",- "BLUEBOOK",- "URBANICITY") %>% filter(df_train_clus$cluster > 1)
df_train_clus <- select(df_train_clus, - "CAR_TYPE", - "OCCUPATION", - "EDUCATION", - "cluster",- "OLDCLAIM",- "BLUEBOOK",- "URBANICITY") %>% filter(df_train_clus$cluster > 2)

#summary(df_train_clus)

df_test_clus <- df_test
df_test_clus$cluster <- km.test
df_test_iv <- select(df_test_clus,- "OLDCLAIM",- "BLUEBOOK",- "URBANICITY") %>% filter(df_test_clus$cluster > 1)
df_test_clus <- select(df_test_clus, - "CAR_TYPE", - "OCCUPATION", - "EDUCATION", - "cluster",- "OLDCLAIM",- "BLUEBOOK",- "URBANICITY") %>% filter(df_test_clus$cluster > 2)
#summary(df_test_clus)


df_validation_clus <- df_validation
df_validation_clus$cluster <- km.validation
df_validation_iv <- select(df_validation_clus,- "OLDCLAIM",- "BLUEBOOK",- "URBANICITY") %>% filter(df_validation_clus$cluster > 1)
df_validation_clus <- select(df_validation_clus, - "CAR_TYPE", - "OCCUPATION", - "EDUCATION", - "cluster",- "OLDCLAIM",- "BLUEBOOK",- "URBANICITY") %>% filter(df_validation_clus$cluster > 1)
#summary(df_validation_clus)

#, - "cluster",  - "CLM_FREQ"
```

```{r echo=FALSE}
df_train <- df_train_clus
df_test <- df_test_clus
df_validation <- df_validation_clus
```
 
## Reduccion de la dimensionalidad.

### PCA

Principal Component Analysis (PCA) es un método estadístico que permite simplificar la complejidad de espacios muestrales con muchas dimensiones a la vez que conserva su información.

Para el analisis de componentes principales vamos a utilizar la BBDD de variables originales sin las tres variables que nos predijeron los cluster que no las vamos a utilizar en la BBDD de los otros dos cluster que vamos a analizar con los modelos.

El primer paso que vamos a dar va a ser generar los componentes principales y ver la desviacion estandar, la proporcion de la varianza y la proporcion acumulativa.

Vemos que con 5 componentes principales explicamos el 50% de la varianza pero que hasta el componente 10 no llegamos a explicar el 90%


```{r echo=FALSE, cache=TRUE}

#df_train_dimen <- df_train_dimen %>% mutate_if(is.integer,as.numeric)
df_train_dimen <- select(df_train, - "CLAIM_FLAG")

df_train_dimen <- df_train_dimen %>% mutate_if(is.integer,as.numeric)


pca_train <- prcomp(x = df_train_dimen, scale = T, center = T)
summary(pca_train)
names(pca_train)
```
```{r echo=FALSE, results='hide'}
pca_train
```
```{r echo=FALSE, results='hide'}
pca_train$center
```

```{r echo=FALSE, results='hide'}
pca_train$scale
```


Lo vemos mejor en la gráfica donde podemos ver que no encontramos ningún factor importante que nos haga ver que podemos hacer una reducción apropiada de la dimensionalidad.

```{r echo=FALSE, warning=FALSE}
#Getting proportion of variance for a scree plot
pr.var <- pca_train$sdev^2
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, 
     xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0,0.2), 
     type = "b")

pve
```

Aqui podemos ver que el primer componente principal explica un 13% de la varianza y poco a poco va disminuyendo lo que nos da que pensar que no se va a poder hacer una reducción apropiada de la dimensionalidad.


```{r echo=FALSE, warning=FALSE}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")
cumsum(pve)
```


### TSNE

Otra de las tecnicas de reducción de la dimensionalidad es stochastic neighbor embedding (tSNE) la cual convierte las distancias euclídeas multidimensionales entre pares de observaciones en probabilidades condicionales.

La ventaja con respecto a PCA es que recoge combinaciones no lineales entre las variables originales.

Utilizamos una perplexity de 30 que nos representa la importancia que le damos a los vecinos.

```{r echo=FALSE, cache=TRUE, message=FALSE, warning= FALSE, result=FALSE}
colors = rainbow(length(unique(train$label)))
names(colors) = unique(train$label)

## Executing the algorithm on curated data
tsne <- Rtsne(df_train_dimen, dims = 2, perplexity=30, verbose=TRUE, max_iter = 500, check_duplicates = FALSE)
# exeTimeTsne<- system.time(Rtsne(train_kmeans, dims = 2, perplexity=30, verbose=TRUE, max_iter = 500))
```

```{r echo=FALSE, cache=TRUE, message=FALSE, warning= FALSE}

resultados <- as.data.frame(tsne$Y)
colnames(resultados) <- c("dim_1", "dim_2")
resultados$numero <- as.character(df_train$CLAIM_FLAG)
ggplot(data = resultados, aes(x = dim_1, y = dim_2)) +
  geom_point(aes(color = numero)) + 
  theme_bw()

```

Como podemos ver en la gráfica no hay una división clara que nos haga ver que podemos utilizar esta técnica para poder reducir la dimensionalidad.

Véase ahora la reducción a un espacio de 3 dimensiones.

```{r echo=FALSE, cache=TRUE, warning=FALSE}
library(scatterplot3d)
library(RColorBrewer)
tsne <- Rtsne(X = df_train_dimen, is_distance = FALSE, dims = 3, perplexity = 30,
              theta = 0.5, max_iter = 500, check_duplicates = FALSE)

resultados <- as.data.frame(tsne$Y)
colnames(resultados) <- c("dim_1", "dim_2", "dim_3")
resultados$numero <- as.factor(df_train$CLAIM_FLAG)

colores <- brewer.pal(n = 2, name = "Set3")
colores <- colores[as.numeric(resultados$numero)]
scatterplot3d(x = resultados$dim_1,
              y = resultados$dim_2,
              z = resultados$dim_3,
              pch = 20, color = colores, cex.lab = 0.8,
              grid = TRUE, box = FALSE)
legend("bottom", legend = levels(resultados$numero),
      col = colores, pch = 16, 
      inset = -0.23, xpd = TRUE, horiz = TRUE)
```

Tampoco hay una división clara que nos anime a pensar que podemos reducir la dimensionalidad.


## Aplicacion de WOE al resto de la BBDD.

Para la aplicación del metodo WOE partimos de las variables originales para ambos cluster y quitamos las tres variables utilizados para la clusterización anterior y añadimos la variable cluster.

Primero vamos a ejecutar el informacion value y nos quedaremos con las variables cuyo valor informativo sea mayor que 0.1.

```{r echo=FALSE}

iv = iv(df_train_iv, y = 'CLAIM_FLAG') %>%
  as_tibble() %>%
  mutate( info_value = round(info_value, 3) ) %>%
  arrange( desc(info_value) )%>% 
  filter(info_value >= 0.1)%>% 
  filter(info_value < 0.55)

iv %>%
  knitr::kable()
```

Posteriormente a esto nos quedamos con las divisiones que hace de cada variable y se lo aplicamos a los tres sets.

```{r echo=FALSE, message=FALSE, warning=FALSE}
bins <- woebin(df_train_iv %>% select(iv$variable, CLAIM_FLAG), y = "CLAIM_FLAG",  positive = "1")

df_train_woe <- woebin_ply(df_train_iv %>% select(iv$variable, CLAIM_FLAG), bins = bins)
df_test_woe <- woebin_ply(df_test_iv %>% select(iv$variable, CLAIM_FLAG), bins = bins)
df_validation_woe <- woebin_ply(df_validation_iv %>% select(iv$variable, CLAIM_FLAG), bins = bins)

df_train_woe_v2 <- df_train_woe
df_test_woe_v2 <- df_test_woe
df_validation_woe_v2 <- df_validation_woe

df_train <- df_train_woe_v2
df_test <- df_test_woe_v2
df_validation <- df_validation_woe

```

```{r echo=FALSE, results=FALSE}
bins$OCCUPATION %>%
  knitr::kable()

bins$EDUCATION %>%
  knitr::kable()
```

Finalmente nos quedamos con un dataset de train con 12 variables con un balanceo de 32%(1) / 68%(0)

```{r echo=FALSE}
df_train$CLAIM_FLAG <- factor(df_train$CLAIM_FLAG, levels = c("0", "1"), labels = c("NOacc", "SIacc"))

df_test$CLAIM_FLAG <- factor(df_test$CLAIM_FLAG, levels = c("0", "1"), labels = c("NOacc", "SIacc"))

df_validation$CLAIM_FLAG <- factor(df_validation$CLAIM_FLAG, levels = c("0", "1"), labels = c("NOacc", "SIacc"))

str(df_train)
```



## Modelo Supervisado

Ahora empezamos con modelos supervisados de clasificación. Para estos modelos partiremos del dataset de train anterior en la que hemos aplicado el WOE.

Vamos a probar GLM, KNN, Decision Tree, random forest, SVM y Naive Bayes.

Para ello vamos a entrenar a la par que calculamos los mejores hiperparametros para cada modelo.

En cada uno de los modelos haremos 9 ejecuciones de cada combinación de parametros que hayamos programado estudiar.

Posteriormente nos quedaremos con el/los parametros que mejor accuracy nos hayan dado y con sus 9 ejecuciones para hacer una media y sacar el valor de accuracy y de Kappa.


### Hiperparametros de GLM

Para el GLM entendemos que el principal parametro son las variables que le aportes al modelo. Para ello hacemos un estudio basado en el R cuadrado ajustado viendo que podemos prescindir de las variables de cluster y homekids. 

Hacemos un lanzamiento sin estas variables y el resultado es minimamente mejor que con toda la BBDD pasando el accuracy de un 72% a un 72.18%.

```{r echo=FALSE, result=FALSE}
regsubsets.out <-
    regsubsets(CLAIM_FLAG ~. ,
               data = df_train,
               nbest = 1,       # 1 best model for each number of predictors
               nvmax = NULL,    # NULL for no limit on number of variables
               force.in = NULL, force.out = NULL,
               method = "exhaustive")
#regsubsets.out
```
```{r echo=FALSE, result=FALSE}
summary.out <- summary(regsubsets.out)
#as.data.frame(summary.out$outmat)
```
```{r echo=FALSE, message=FALSE}
plot(regsubsets.out, scale = "adjr2", main = "Adjusted R^2")
```

```{r echo=FALSE, warning=FALSE}
# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)

particiones  <- 3
repeticiones <- 3

# Hiperparámetros
hiperparametros <- data.frame(parameter = "none")

set.seed(342)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)

modelo_logistic <- train(CLAIM_FLAG ~ ., data = df_train,
                         method = "glm",
                         tuneGrid = hiperparametros,
                         metric = "Accuracy",
                         trControl = control_train,
                         family = "binomial")
modelo_logistic
```


### Hiperparametros de KNN

Para el knn parametrizamos diferente numero de vecinos minimo 5 y maximo 200.

```{r echo=FALSE, cache=TRUE}
particiones  <- 3
repeticiones <- 3

# Hiperparámetros
hiperparametros <- data.frame(k = c(5, 10 ,30, 50, 100, 200))

set.seed(342)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros)) 
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_knn <- train(CLAIM_FLAG ~ ., data = df_train,
                    method = "knn",
                    tuneGrid = hiperparametros,
                    metric = "Accuracy",
                    trControl = control_train)
modelo_knn
```

Como vemos en la gráfica nos quedaremos con 100 pese a que a partir de 30 la mejora es minima. Si tuvieramos problemas computacionales sería un tema a tener en cuenta.

```{r echo=FALSE}
plot(modelo_knn)
```

### Hiperparametros de Decision Tree

El hiperparametro a ajustar en el decision tree es el parametro de complejidad el cual nos controla el tamaño del arbol de decisión.

```{r echo=FALSE, cache=TRUE}
# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
particiones  <- 3
repeticiones <- 3

# Hiperparámetros
#hiperparametros <- data.frame(parameter = "none")
hiperparametros <- data.frame(cp = c(0.01, 0.011, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017, 0.018, 0.019))
set.seed(342)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)


# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_C50Tree <- train(CLAIM_FLAG ~ ., data = df_train,
                    method = "rpart",
                    tuneGrid = hiperparametros,
                    metric = "Accuracy",
                    trControl = control_train)
modelo_C50Tree
```

En nuestro caso nos quedamos con el parametro mas pequeño de los probados.

```{r echo=FALSE}
plot(modelo_C50Tree)
```


### Hiperparametros de Random Forest

Para el random forest probamos el numero de variables para un arbol y el tamaño minimo del nodo dejando fijo el numero de arboles en 500.

```{r echo=FALSE, cache=TRUE}
# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
particiones  <- 3
repeticiones <- 3

# Hiperparámetros
hiperparametros <- expand.grid(mtry = c(2,4,8),
                               min.node.size = c(15, 20, 30),
                               splitrule = "gini")

set.seed(342)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_rf <- train(CLAIM_FLAG ~ ., data = df_train,
                   method = "ranger",
                   tuneGrid = hiperparametros,
                   metric = "Accuracy",
                   trControl = control_train,
                   # Número de árboles ajustados
                   num.trees = 500)
modelo_rf
```

Como vemos en la gráfica el tamaño minimo sería de 30 y el número de variables de 4

```{r echo=FALSE}
plot(modelo_rf)
```

### Hiperparametros de SVM

Para el SVM parametrizaremos sigma y C.

```{r echo=FALSE, cache=TRUE}
# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
particiones  <- 3
repeticiones <- 3


# Hiperparámetros
hiperparametros <- expand.grid(sigma = c(0.001, 0.01),
                               C = c(100, 200))

set.seed(342)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_svmrad <- train(CLAIM_FLAG ~ ., data = df_train,
                   method = "svmRadial",
                   tuneGrid = hiperparametros,
                   metric = "Accuracy",
                   trControl = control_train)
modelo_svmrad
```

Como vemos en la gráfica nos quedaremos con C = 100 y con sigma igual a 0.01

```{r echo=FALSE}
plot(modelo_svmrad)
```

### Hiperparametros de Redes Bayesianas

Finalmente para las redes bayesianas no hacemos pruebas con ningún parametro obteniendo un accuracy del 70%.

```{r echo=FALSE, cache=TRUE}
# PARALELIZACIÓN DE PROCESO
#===============================================================================
#library(doMC)
#registerDoMC(cores = 4)

# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
particiones  <- 3
repeticiones <- 3

# Hiperparámetros
hiperparametros <- data.frame(usekernel = FALSE, fL = 0 , adjust = 0)

#hiperparametros <- data.frame(smooth = 1)


set.seed(123)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_nb <- train(CLAIM_FLAG ~ ., data = df_train,
                   method = "nb",
                   tuneGrid = hiperparametros,
                   metric = "Accuracy",
                   trControl = control_train)
modelo_nb
```

### Comparativa de modelos

Primero vamos a analizar las medias de las valores que nos ha dado nuestro lanzamiento en train para los hiperparametros que han sido definitivamente los que mas accuracy han obtenido en cada modelo.

```{r echo=FALSE, cache=TRUE}
modelos <- list(KNN = modelo_knn, logistic = modelo_logistic,
                arbol = modelo_C50Tree, rf = modelo_rf, SVMRadial = modelo_svmrad, NB = modelo_nb)

resultados_resamples <- resamples(modelos)
metricas_resamples <- resultados_resamples$values %>%
                         gather(key = "modelo", value = "valor", -Resample) %>%
                         separate(col = "modelo", into = c("modelo", "metrica"),
                                  sep = "~", remove = TRUE)
metricas_resamples %>% 
  group_by(modelo, metrica) %>% 
  summarise(media = mean(valor)) %>%
  spread(key = metrica, value = media) %>%
  arrange(desc(Accuracy))

metricas_resamples %>% 
  group_by(modelo, metrica) %>% 
  summarise(media = mean(valor)) %>%
  ggplot(aes(x = reorder(modelo, media), y = media, label = round(media, 2))) +
    geom_segment(aes(x = reorder(modelo, media), y = 0,
                     xend = modelo, yend = media),
                     color = "grey50") +
    geom_point(size = 7, color = "firebrick") +
    geom_text(color = "white", size = 2.5) +
    scale_y_continuous(limits = c(0, 1)) +
    labs(title = "Validación: Accuracy medio repeated-CV",
         subtitle = "Modelos ordenados por media",
         x = "modelo") +
    coord_flip() +
    theme_bw()
```


Como vemos el modelo de Ramdon forest, el logistico, el SVM y el KNN están practicamente empatados y se diferencian en el Kappa siendo el random forest el que mas tiene entre estos..

Vamos a verlo de manera gráfica como se distribuyen los valores para cada modelo de las 9 simulaciones realizadas.

```{r echo=FALSE}
metricas_resamples %>% filter(metrica == "Accuracy") %>%
  group_by(modelo) %>% 
  mutate(media = mean(valor)) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(modelo, media), y = valor, color = modelo)) +
    geom_boxplot(alpha = 0.6, outlier.shape = NA) +
    geom_jitter(width = 0.1, alpha = 0.6) +
    scale_y_continuous(limits = c(0, 1)) +
    
    # Accuracy basal
    #geom_hline(yintercept = 0.62, linetype = "dashed") +
    #annotate(geom = "text", y = 0.6, x = 4.4, label = "Accuracy basal") +
    theme_bw() +
    labs(title = "Validación: Accuracy medio repeated-CV",
         subtitle = "Modelos ordenados por media") +
    coord_flip() +
    theme(legend.position = "none")
```

En principio podemos decir que nos parece con mayor calidad el random forest ya que tiene el diagrama de caja mas estrecho aun teniendo algun outlier.

Para asegurarnos que podriamos diferenciarlos vamos a hacer el test de WILCOXON. Este test es un estadistico que nos ayudará a ver si son significativamente diferente las medias de los modelos.


### Test de RANGOS DE WILCOXON.

La prueba de los rangos con signo de Wilcoxon es una prueba no paramétrica para comparar el rango medio de dos muestras relacionadas y determinar si existen diferencias entre ellas.


```{r echo=FALSE, warning = FALSE}
# Comparaciones múltiples con un test suma de rangos de Wilcoxon
# ==============================================================================

metricas_accuracy <- metricas_resamples %>% filter(metrica == "Accuracy")
comparaciones  <- pairwise.wilcox.test(x = metricas_accuracy$valor, 
                                        g = metricas_accuracy$modelo,
                                        paired = TRUE,
                                        p.adjust.method = "holm")

# Se almacenan los p_values en forma de dataframe
comparaciones <- comparaciones$p.value %>%
  as.data.frame() %>%
  rownames_to_column(var = "modeloA") %>%
  gather(key = "modeloB", value = "p_value", -modeloA) %>%
  na.omit() %>%
  arrange(modeloA)

comparaciones

```

Acorde a las comparaciones por pares, no existen evidencias suficientes para considerar que la capacidad predictiva de los modelos Random Forest, SVM radial y logistica es distinta. 

Por ultimo vamos a comparar todos los modelos utilizando el set de test y la curva ROC.

## Predicion con el dataset de test y Curva ROC

### Prediccion con dataset del modelo.

Ahora vamos a aplicar la predicción de cada modelo al dataset de test y ver como se comportan.

```{r echo=FALSE, warning= FALSE}
set.seed(342)

pred_prob_glm_final <- predict(modelo_logistic, newdata = df_test)
pred_prob_svm_final <- predict(modelo_svmrad, newdata = df_test)
pred_prob_rf_final <- predict(modelo_rf, newdata = df_test)
pred_prob_knn_final <- predict(modelo_knn, newdata = df_test)
pred_prob_Tree_final <- predict(modelo_C50Tree, newdata = df_test)
pred_prob_nb_final <- predict(modelo_nb, newdata = df_test)

paste("El Accuracy de test del modelo GLM:", round(confusionMatrix(pred_prob_glm_final, df_test$CLAIM_FLAG)$overall[1]*100, 3), "%")
paste("El Accuracy de test del modelo SVM:", round(confusionMatrix(pred_prob_svm_final, df_test$CLAIM_FLAG)$overall[1]*100, 3), "%")
paste("El Accuracy de test del modelo rf:", round(confusionMatrix(pred_prob_rf_final, df_test$CLAIM_FLAG)$overall[1]*100, 3), "%")
paste("El Accuracy de test del modelo knn:", round(confusionMatrix(pred_prob_knn_final, df_test$CLAIM_FLAG)$overall[1]*100, 3), "%")
paste("El Accuracy de test del modelo tree:", round(confusionMatrix(pred_prob_Tree_final, df_test$CLAIM_FLAG)$overall[1]*100, 3), "%")
paste("El Accuracy de test del modelo NB:", round(confusionMatrix(pred_prob_nb_final, df_test$CLAIM_FLAG)$overall[1]*100, 3), "%")

paste("El Kappa de test del modelo GLM:", round(confusionMatrix(pred_prob_glm_final, df_test$CLAIM_FLAG)$overall[2]*100, 3), "%")
paste("El Kappa de test del modelo SVM:", round(confusionMatrix(pred_prob_svm_final, df_test$CLAIM_FLAG)$overall[2]*100, 3), "%")
paste("El Kappa de test del modelo rf:", round(confusionMatrix(pred_prob_rf_final, df_test$CLAIM_FLAG)$overall[2]*100, 3), "%")
paste("El Kappa de test del modelo knn:", round(confusionMatrix(pred_prob_knn_final, df_test$CLAIM_FLAG)$overall[2]*100, 3), "%")
paste("El Kappa de test del modelo tree:", round(confusionMatrix(pred_prob_Tree_final, df_test$CLAIM_FLAG)$overall[2]*100, 3), "%")
paste("El Kappa de test del modelo NB:", round(confusionMatrix(pred_prob_nb_final, df_test$CLAIM_FLAG)$overall[2]*100, 3), "%")

```

Vemos que el resultado de test es muy parecido al de train destacando levemente el KNN con respecto al resto.

### Curva ROC

Ahora vamos a comparar las curvas ROC para ver como de diferentes son y poder decidir definitivamente con que modelo nos quedamos.

Viendo la imagen podriamos decir que la curva ROC que mas nos convence es la del KNN aunque muy solapada con la del GLM.

```{r echo=FALSE}
pred1 <- prediction(as.numeric(pred_prob_rf_final), as.numeric(df_test$CLAIM_FLAG))

pred2 <- prediction(as.numeric(pred_prob_svm_final), as.numeric(df_test$CLAIM_FLAG))

pred3 <- prediction(as.numeric(pred_prob_glm_final), as.numeric(df_test$CLAIM_FLAG))

pred4 <- prediction(as.numeric(pred_prob_knn_final), as.numeric(df_test$CLAIM_FLAG))

pred5 <- prediction(as.numeric(pred_prob_Tree_final), as.numeric(df_test$CLAIM_FLAG))

pred6 <- prediction(as.numeric(pred_prob_nb_final), as.numeric(df_test$CLAIM_FLAG))

perf1 <- performance(pred1, "tnr", "tpr")
perf2 <- performance(pred2, "tnr", "tpr")
perf3 <- performance(pred3, "tnr", "tpr")
perf4 <- performance(pred4, "tnr", "tpr")
perf5 <- performance(pred5, "tnr", "tpr")
perf6 <- performance(pred6, "tnr", "tpr")

plot(perf1, col="blue", xlim=c(1,0))
plot(perf2,col="purple", add=TRUE)
plot(perf3,col="orange", add=TRUE)
plot(perf4,col="red", add=TRUE)
plot(perf5,col="green", add=TRUE)
plot(perf6,col="grey", add=TRUE)

legend(x="right", legend=c("RF", "SVM", "GLM", "KNN", "TREE"), fill =c("blue", "purple", "orange", "red", "green"), cex=0.8)
```

# Validacion

Por ultimo vamos a utilizar el set de Validación sobre el modelo que mas robustez nos da que en este caso parece ser el KNN.

Aplicamos entonces este set de validación sobre el KNN obteniendo los siguientes resultados.

```{r echo=FALSE}
set.seed(342)

val_prob_log_final <- predict(modelo_knn, newdata = df_validation)

paste("El Accuracy de validacion del modelo KNN:", round(confusionMatrix(val_prob_log_final, df_validation$CLAIM_FLAG)$overall[1]*100, 3), "%")

paste("El Kappa de validacion del modelo KNN:", round(confusionMatrix(val_prob_log_final, df_validation$CLAIM_FLAG)$overall[2]*100, 3), "%")

```


# Conclusiones y Trabajo Futuro.

A la hora de la venta de nuestro trabajo a una compañia de seguros podríamos decir lo siguiente:

1. Con unicamente tres variables de la persona (Si ha tenido siniestros anteriormente, Valor del vehiculo y Si conduce por ciudad) seriamos capaz de identificar un porcentaje de siniestralidad para esa persona siendo uno de ellos practicamente un cero.

2. Si quisieramos profundizar en los cluster que no son cero tendriamos un accuracy de un 75%.

3. En el total de la cartera (entendida cartera como set de train) tendriamos un accuracy del 80% (75% de un volumen del 80% de la cartera y un 100% de un volumen de 20% de la cartera)

Como trabajo futuro nos gustaría ahondar mas en la separación de los dos cluster que hemos llevado a los modelos ya que pensamos que si lo hacemos de manera separada podemos personalizar todavía más la tarificación y dar un modelo más ajustado.





