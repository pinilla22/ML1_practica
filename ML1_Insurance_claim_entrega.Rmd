---
title: "Insurance Analysis"
author: 
- Juan Manuel Jiménez
- Javier Pinilla
- Luis Rus
date: "12/16/2019"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: flatly
    code_folding: hide
---

# Objetivo del Proyecto

El objetivo del proyecto es poder identificar que factores de los asegurados afectan en la siniestralidad de las aseguradoras de autos. 

La siniestralidad afecta de lleno en la cuenta de resultados de la aseguradora y una buena comprensión de estos factores hará que la compañía pueda cuantificar bien el riesgo y por lo tanto el importe a cobrar al asegurado e incluso que pueda rechazar ciertos riesgos que no quiera asumir.

Contamos con un dataset que contiene informacion sobre el tomador y el vehículo a asegurar. La variable objetivo es en este caso CLAIM FLAG, nos indica si ha tenido acciente o no.


El dataset tiene 27 variables que vamos a explicar a continuación:

**ID** - ID de poliza

**KIDSDRIV** - Numero de adolescentes con licencia de conducir en la unidad familiar.

**BIRTH** -	Año de nacimiento.

**AGE** -	Edad.

**HOMEKIDS** - Adolescentes viviendo en la casa de la unidad familiar.

**YOJ** - Años en el trabajo.

**INCOME** - Ingresos anuales.

**PARENT1** - Flag de Familia uniparental.

**HOME_VAL**- Valor de la vivienda.

**MSTATUS**- Estado Civil (Soltero-Casado).

**GENDER**- Sexo.

**EDUCATION**- Nivel de educación.

**OCCUPATION**-	Ocupacion.

**TRAVTIME**- Tiempo diario en minutos al centro de trabajo.

**CAR_USE**- Uso del vehiculo.

**BLUEBOOK**- Valor actual del vehiculo.

**TIF**- Tiempo en vigor en la compañía actual.

**CAR_TYPE**- Tipo de vehiculo.

**RED_CAR**- Flag Vehiculo de color Rojo.

**OLDCLAIM**- Valor de los siniestros anteriores al año en curso.

**CLM_FREQ**- Frecuencia de los siniestros anteriores al año en curso.

**REVOKED**- Siniestro revocado por la compañia.

**MVR_PTS**- Nº de puntos retirados del carnet.

**CLM_AMT**- Cuantia del siniestro en el año en curso.

**CAR_AGE**- Años del vehiculo.

**CLAIM_FLAG**- Flag de siniestro año en curso.

**URBANICITY**- Lugar de conduccion - Urbano vs. Rural

Para poder realizar el proyecto vamos a necesitar las siguientes librerías. Algunas de estas librerías son específicas para el tratamiento de nulos, para la representación gráfica o para el desarrollo de modelos.


```{r message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(egg)
library(GGally)
library(ISLR)
library(car)
library(DMwR2)
library(faraway)
library(mlbench)
library(knitr)
library(kableExtra)
library(htmltools)
library(bsplus)
library(RColorBrewer)
library(lattice)
library(VIM)
library(mice)
library(stringr)
library(corrplot)
library(tidyverse)
library(lsr)
library(cowplot)
library(tidymodels)
library(skimr)
library(tibble)
library(lubridate)
library(padr)
library(caret)
library(partykit)
library(DMwR)
library(lime)
library(parsnip)
library(randomForest)
library(cluster)
library(RColorBrewer)
library(factoextra)
library(readr)
library(dplyr)
library(ggplot2)
library(stringr)
library(Rtsne)
library(pROC)
library(ggpubr)
library(ROCR)
library(rpart)
library(rpart.plot)
library(e1071)
```

# Estudio exploratorio de los datos

```{r}
insurance <- read.csv(file="car_insurance_claim.csv", header=TRUE, sep=",")
dim(insurance)
```

```{r}
set.seed(101)

trainIndex <- createDataPartition(insurance$CLAIM_FLAG, p = .75, 
                                  list = FALSE, 
                                  times = 1)
train <- insurance[trainIndex,]
test <- insurance[-trainIndex,]

# Contamos el numero de líneas para el análisis.
n_train <- nrow(train)
n_test <- nrow(test)
```

# Limpieza de los datos

## Transformaciones de variables cuantitativas

```{r}
# Cambiar "," por "." y eliminar los $
train$INCOME <- str_replace_all(train$INCOME, ",", "")
train$INCOME <- str_sub(train$INCOME, 2, -1)
train$HOME_VAL <- str_replace_all(train$HOME_VAL, ",", "")
train$HOME_VAL <- str_sub(train$HOME_VAL, 2, -1)
train$BLUEBOOK <- str_replace_all(train$BLUEBOOK, ",", "")
train$BLUEBOOK <- str_sub(train$BLUEBOOK, 2, -1)
train$OLDCLAIM <- str_replace_all(train$OLDCLAIM, ",", "")
train$OLDCLAIM <- str_sub(train$OLDCLAIM, 2, -1)
train$CLM_AMT <- str_replace_all(train$CLM_AMT, ",", "")
train$CLM_AMT <- str_sub(train$CLM_AMT, 2, -1)
```

```{r}
# Cambiar variables tipo "character" a "numeric"
train$INCOME <- as.numeric(train$INCOME)
train$HOME_VAL <- as.numeric(train$HOME_VAL)
train$BLUEBOOK <- as.numeric(train$BLUEBOOK)
train$OLDCLAIM <- as.numeric(train$OLDCLAIM)
train$CLM_AMT <- as.numeric(train$CLM_AMT)
sapply(train, class)
```

## Procesado de variables cualitativas

```{r warning=FALSE}
# Quitar prefijo z_
train <- train %>% mutate_if(is.factor,funs(str_replace(.,"z_", "")))
sapply(train, class)
```

```{r}
train$OCCUPATION <- factor(train$OCCUPATION,
                          ordered = FALSE,
                          levels = c("", "Clerical", "Doctor", "Home Maker", "Lawyer", "Manager", "Professional", "Student", "Blue Collar"),
                          labels = c("Others", "Clerical", "Doctor", "Home Maker", "Lawyer", "Manager", "Professional", "Student", "Blue Collar"))
```

```{r}
# Pasar columnas tipo "character" a "factor"
train <- train %>% mutate_if(is.character, as.factor)
sapply(train, class)
```

```{r}
# Creamos una tabla con las variables y los valores que transformaremos con el valor 1
variables_binarias <- data.frame(Variable = c("CAR_USE","MSTATUS","PARENT1","RED_CAR","REVOKED","GENDER","URBANICITY"),
	valor_1 = c("Commercial","Yes","Yes","yes","Yes","M","Highly Urban/ Urban"),
	stringsAsFactors=FALSE)
for(i in 1:length(variables_binarias$Variable)){
  var <- variables_binarias$Variable[i]
  true_value <- variables_binarias$valor_1[i]
  train[,var] <- ifelse(train[,var] == true_value,1,0)
}
```

### Imputación de variables

```{r, echo=FALSE, cache=TRUE}
# INCOME
stoch_imp <- train %>% VIM::kNN(., variable=c("INCOME","BLUEBOOK"))
d1 = train$INCOME
train$INCOME <- stoch_imp$INCOME
x <- data.frame(d1 = d1, d2 = stoch_imp$INCOME)
data <- melt(x)
ggplot(data,aes(x=value, fill=variable)) + geom_density(alpha=0.25)

# CAR_AGE
stoch_imp <- train %>% VIM::kNN(., variable=c("CAR_AGE", "INCOME","EDUCATION"))
d1 = train$CAR_AGE
train$CAR_AGE <- stoch_imp$CAR_AGE
x <- data.frame(d1 = d1, d2 = stoch_imp$CAR_AGE)
data <- melt(x)
ggplot(data,aes(x=value, fill=variable)) + geom_density(alpha=0.25)
train <- train[train$CAR_AGE > -1,]

# YOJ
stoch_imp <- train %>% VIM::kNN(., variable=c("INCOME","YOJ","EDUCATION", "HOMEKIDS", "AGE"))
d1 = train$YOJ
train$YOJ <- stoch_imp$YOJ
x <- data.frame(d1 = d1, d2 = stoch_imp$YOJ)
data <- melt(x)
ggplot(data,aes(x=value, fill=variable)) + geom_density(alpha=0.25)

# AGE
train$BIRTH <- as.character(train$BIRTH)
train <- train %>% mutate(AGE=ifelse(is.na(AGE), 120-as.numeric(substr(BIRTH, nchar(BIRTH)-1, nchar(BIRTH))), AGE))

# HOME_VAL
stoch_imp <- train %>%  VIM::kNN(., variable=c("INCOME","HOME_VAL"), k=1)
train$INCOME <- stoch_imp$INCOME
train$HOME_VAL <- stoch_imp$HOME_VAL
```

```{r echo=FALSE}
train$bin_OLDCLAIM <- ifelse(train$OLDCLAIM > 0,1,0)
train$bin_HOMEKIDS <- ifelse(train$HOMEKIDS > 3,1,0)
train["bin_KIDSDRIV"] <- ifelse(train["KIDSDRIV"] > 0,1,0)
train["bin_TIF"] <- ifelse(train["TIF"] < 3 | train["TIF"] > 15 ,0,1)
train["bin_AGE"] <- ifelse(train["AGE"] < 26 | train["AGE"] > 57 ,1,0)
train["bin_MVR_PTS"] <- ifelse(train["MVR_PTS"] > 6,1,0)
train["bin_OCCUPATION"] <- ifelse(train["OCCUPATION"] == "Lawyer" | train["OCCUPATION"] == "Manager"| train["OCCUPATION"] == "Doctor" ,0,1)
train["bin_EDUCATION"] <- ifelse(train["EDUCATION"] == "1-<High School" | train["EDUCATION"] == "2-High School",1,0)
train["bin_CAR_TYPE"] <- ifelse(train["CAR_TYPE"] == "Minivan",0,1)
# Variables logarítmicas
train$CLM_AMT_log <- log(train$CLM_AMT,10)
train$INCOME_log <- log(train$INCOME+1,10)
train$BLUEBOOK_log <- log(train$BLUEBOOK,10)
train$OLDCLAIM_log <- log(train$OLDCLAIM+1,10)
```

```{r echo=FALSE}
train$EDUCATION <- factor(train$EDUCATION,
                          ordered = TRUE,
                          levels = c("<High School", "High School", "Bachelors", "Masters", "PhD"),
                          labels = c("1-<High School", "2-High School", "3-Bachelors", "4-Masters", "PhD"))
```

# MODIFICACIONES PARA EL SUBSET DE TEST

```{r}
# Cambiar "," por "." y eliminar los $
test$INCOME <- str_replace_all(test$INCOME, ",", "")
test$INCOME <- str_sub(test$INCOME, 2, -1)
test$HOME_VAL <- str_replace_all(test$HOME_VAL, ",", "")
test$HOME_VAL <- str_sub(test$HOME_VAL, 2, -1)
test$BLUEBOOK <- str_replace_all(test$BLUEBOOK, ",", "")
test$BLUEBOOK <- str_sub(test$BLUEBOOK, 2, -1)
test$OLDCLAIM <- str_replace_all(test$OLDCLAIM, ",", "")
test$OLDCLAIM <- str_sub(test$OLDCLAIM, 2, -1)
test$CLM_AMT <- str_replace_all(test$CLM_AMT, ",", "")
test$CLM_AMT <- str_sub(test$CLM_AMT, 2, -1)
# Cambiar variables tipo "character" a "numeric"
test$INCOME <- as.numeric(test$INCOME)
test$HOME_VAL <- as.numeric(test$HOME_VAL)
test$BLUEBOOK <- as.numeric(test$BLUEBOOK)
test$OLDCLAIM <- as.numeric(test$OLDCLAIM)
test$CLM_AMT <- as.numeric(test$CLM_AMT)
# Quitar prefijo z_
test <- test %>% mutate_if(is.factor,funs(str_replace(.,"z_", "")))
# Renombrado labels variable OCCUPATION
test$OCCUPATION <- factor(test$OCCUPATION,
                          ordered = FALSE,
                          levels = c("", "Clerical", "Doctor", "Home Maker", "Lawyer", "Manager", "Professional", "Student", "Blue Collar"),
                          labels = c("Others", "Clerical", "Doctor", "Home Maker", "Lawyer", "Manager", "Professional", "Student", "Blue Collar"))
test$EDUCATION <- factor(test$EDUCATION,
                          ordered = TRUE,
                          levels = c("<High School", "High School", "Bachelors", "Masters", "PhD"),
                          labels = c("1-<High School", "2-High School", "3-Bachelors", "4-Masters", "PhD"))
# Pasar columnas tipo "character" a "factor"
test <- test %>% mutate_if(is.character, as.factor)
# Creamos una tabla con las variables y los valores que transformaremos con el valor 1
for(i in 1:length(variables_binarias$Variable)){
  var <- variables_binarias$Variable[i]
  true_value <- variables_binarias$valor_1[i]
  test[,var] <- ifelse(test[,var] == true_value,1,0)
}
# Imputación de datos faltantes
train_knn_imp <- train %>% select(INCOME, BLUEBOOK, CAR_AGE, EDUCATION, YOJ, HOMEKIDS, AGE)
test_knn_imp <- test %>% select(INCOME, BLUEBOOK, CAR_AGE, EDUCATION, YOJ, HOMEKIDS, AGE)
train_knn_imp$isTrain <- rep(1, nrow(train_knn_imp))
test_knn_imp$isTrain <- rep(0, nrow(test_knn_imp))
train_test_knn_imp <- rbind(train_knn_imp,test_knn_imp)
sapply(test, function(x) sum(is.na(x)))
# Imputado de missings varible INCOME
knn_income_imp <- train_test_knn_imp %>% VIM::kNN(., variable=c("INCOME","BLUEBOOK")) %>% filter(isTrain==0)
test$INCOME <- knn_income_imp$INCOME
# Imputado de missings varible CAR_AGE
knn_carage_imp <- train_test_knn_imp %>% VIM::kNN(., variable=c("CAR_AGE", "INCOME","EDUCATION")) %>%   filter(isTrain==0)
test$CAR_AGE <- knn_carage_imp$CAR_AGE
# Imputado de missings varible YOJ
knn_yoj_imp <- train_test_knn_imp %>% VIM::kNN(., variable=c("INCOME","YOJ","EDUCATION", "HOMEKIDS", "AGE")) %>% filter(isTrain==0)
test$YOJ <- knn_yoj_imp$YOJ
# Imputado de missings variable AGE
test$BIRTH <- as.character(test$BIRTH)
test <- test %>% mutate(AGE=ifelse(is.na(AGE), 120-as.numeric(substr(BIRTH, nchar(BIRTH)-1, nchar(BIRTH))), AGE))
sapply(test, function(x) sum(is.na(x)))
test["education_bin"] <- ifelse(test["EDUCATION"] == "1-<High School" | test["EDUCATION"] == "2-High School",1,0)
test_target <- test
test_target$CLM_AMT_log <- log(test$CLM_AMT,10)
test_target$BLUEBOOK_log <- log(test$BLUEBOOK,10)
test_target$OLDCLAIM_log <- log(test$OLDCLAIM+1,10)
```


## Modelos para la predicción de la variable CLAIM FLAG

Como comentabamos en el apartado anterior vamos a aplicar los modelos de clasificación tanto supervisados como no supervisados.

Primero vamos a preparar la BBDD para realizar los cálculos seleccionando las variables y dando labels a la variable objetivo de NOacc y SIacc dependiendo de si es 0 o 1.

```{r echo=FALSE}
train_random_forest <- train %>% select("OLDCLAIM", "HOMEKIDS", "KIDSDRIV","TIF","AGE","MVR_PTS",
"OCCUPATION","EDUCATION","CAR_TYPE","CLAIM_FLAG", "YOJ","PARENT1","GENDER", "TRAVTIME","CAR_USE","CAR_TYPE","RED_CAR","CLM_FREQ", "URBANICITY", "BLUEBOOK",)

train_kmeans <- train_random_forest %>% mutate_if(is.factor,as.numeric) %>% mutate_if(is.integer,as.numeric)

train_random_forest$CLAIM_FLAG <- factor(train_random_forest$CLAIM_FLAG, levels = c("0", "1"), labels = c("NOacc", "SIacc"))
```

```{r echo=FALSE}
test_random_forest <- test_target %>% select("OLDCLAIM", "HOMEKIDS", "KIDSDRIV","TIF","AGE","MVR_PTS",
"OCCUPATION","EDUCATION","CAR_TYPE","CLAIM_FLAG", "YOJ","PARENT1","GENDER", "TRAVTIME","CAR_USE","CAR_TYPE","RED_CAR","CLM_FREQ", "URBANICITY", "BLUEBOOK",)

test_kmeans <- test_random_forest %>% mutate_if(is.factor,as.numeric) %>% mutate_if(is.integer,as.numeric)

test_random_forest$CLAIM_FLAG <- factor(test_random_forest$CLAIM_FLAG, levels = c("0", "1"), labels = c("NOacc", "SIacc"))
```

##MODELO NO SUPERVISADO

Con los modelos no supervisados vamos a intentar clasificar por grupos los registros para cuando lleguen nuevos registros poder imputarlos a un grupo de los seleccionados.

Con este metodo podemos ver si en los distintos cluster tenemos distintas probabilidades de siniestro y de esa manera podríamos tener distintas tarifas dependiendo del grupo en el que cayeran los nuevos registros.

###Modelo no supervisado KMEANS


Este modelo esta basado en la minimización de la suma de distancias entre cada objeto y el centroide de su grupo o cluster.

Al ser un modelo de distancias es necesario escalar todas las variables. Para ello utilizamos la sentencia Scale.

Al aplicar esto aparecen ciertos valores sin sentido como edades negativas pero no tiene importancia ya que unicamente vamos a ver distancias.


```{r echo=FALSE}
xiris <- scale(train_kmeans)
```

Primero vamos a buscar el punto optimo de cluster en funcion del minimo error.
 
 Vemos en la figura que la pendiente varía levemente entre diferentes numero de cluster y a partir de 5 no varía sustancialmente por lo que elegiremos 5 como numero de centroides. 
 
```{r echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# Initialize total within sum of squares error: wss
wss <- 0
wsr <- 0
set.seed(342)
# For 1 to 15 cluster centers
for (i in 1:10) {
  km.out <- kmeans(xiris, centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
  wsr[i] <- km.out$betweenss
}

# Plot total within sum of squares vs. number of clusters
plot(1:10, wss, type = "b", 
     xlab = "Número de Clusters", 
     ylab = "Suma de cuadrados DENTRO DE grupos")

plot(1:10, wsr, type = "b", 
     xlab = "Número de Clusters", 
     ylab = "suma Cuadrados ENTRE grupos")

```
Estas caracteristicas de kmeans nos da una idea de la calidad de nuestro modelo. Cuanto menor sea la suma de cuadrados dentro de los grupos y mayor sea entre los grupos mejor será el modelo.

```{r echo=FALSE}
print("Los registros para cada cluster son los siguientes:")
km.out$size
print("La suma de cuadrados total es")
km.out$totss
print("La suma de cuadrados entre grupos es")
km.out$betweenss
print("La suma de cuadrados dentro del grupos es")
km.out$withinss
print("La suma de cuadrados agrupada de las medidas dentro del grupos es")
km.out$tot.withinss
print("La Varianza explicada con cinco clusters es")
(km.out$betweenss/km.out$totss) * 100
```


Posteriormente al elegir el numero de centroides vamos a ejecutar el modelo y ver las medias de cada variable en cada Cluster y de este modo poder ver si hay diferencias en la siniestralidad (Variable CLAIM FLAG)

```{r echo=FALSE, cache=TRUE}
set.seed(342)
km <- kmeans(xiris, 5, nstart = 1)
km
aggregate(train_kmeans, by=list(km$cluster), mean)


plot(train_kmeans$AGE, train_kmeans$CLAIM_FLAG, col=km$cluster, xlab="age", ylab="claim_flag")
table(km$cluster, train_kmeans$CLAIM_FLAG)
```

Lo que podemos ver que en los diferentes cluster es que tenemos 3 medias diferenciadas en los 5 cluster de la variable CLAIM_FLAG.

Podriamos decir que tenemos un cluster sin siniestralidad, otros dos con una siniestralidad de un 26% y otros dos con una siniestralidad del 50%.

Si nos fijamos en el numero de registros por cluster un 60% esta en la tarifa del 26% mientras que el otro 40% estaría repartido en 20% buenos y 20% malos conductores.


```{r cache=TRUE}
set.seed(342)

xiris.test <- scale(test_kmeans)
km.test <- kmeans(xiris.test, 5, nstart = 1)
#km.test
#aggregate(train_kmeans$CLAIM_FLAG, by=list(km$cluster), FUN = mean)
#aggregate(test_kmeans$CLAIM_FLAG, by=list(km.test$cluster), mean)

#prop.table(table(km$cluster, train_kmeans$CLAIM_FLAG)) %>% round(digits = 2)
#prop.table(table(km.test$cluster, test_kmeans$CLAIM_FLAG)) %>% round(digits = 2)

table_train <- prop.table(table(km$cluster, train_kmeans$CLAIM_FLAG)) %>% round(digits=2)
colnames(table_train) <- c("0.train", "1.train")
table_train

table_test <- prop.table(table(km.test$cluster, test_kmeans$CLAIM_FLAG)) %>% round(digits=2)
colnames(table_test) <- c("0.test", "1.test")
table_test

clusters <- 5
matplot(1:clusters , cbind(km$withinss,km.test$withinss), pch=19 , col=c("red","blue"),type="b",ylab="Dist. DENTRO DE grupos",xlab="Numbers of Centroids")
legend("topleft",legend=c("Withinss Train","Withinss Test"),pch=19, col=c("red","blue"))
```


###Modelo no supervisado HIERARCHICAL CLUSTERING.

```{r echo=FALSE, cache=TRUE}
clusters <- hclust(dist(train_kmeans))
plot(clusters)
```
Como podemos ver en el dendograma una división apropiada podría ser 6.

```{r echo=FALSE, cache=TRUE}
clusterCut <- cutree(clusters, 6)
table(clusterCut, train_kmeans$CLAIM_FLAG)
```
 
 Como vemos los clusters creados por el metodo de hierarchical clustering no nos dan una diferenciación clara en base a la siniestralidad de la cartera
 
## MODELO SUPERVISADO

Ahora empezamos con modelos supervisados de clasificación. Estos modelos tienen en cuenta la variable objetivo que en nuestro caso es CLAIM_FLAG.

Vamos a probar el random forest, GLM, KNN, Decision Tree y SVM

###RANDOM FOREST

El modelo random forest nos va a permitir obtener una clasificación de clientes en las clases de tendrá accidente o no tendrá accidente.

Para ello utilizamos la funcion train del paquete caret que nos permite amoldar la formula al modelo y con los distintos parametros además de darnos una pequeña selección de valores de los parametros que no definamos y quedarse con los que mejor accuracy tiene par ala parte de test.

```{r echo=FALSE, cache=TRUE}
set.seed(342)
modelo_rf <- train(CLAIM_FLAG ~ ., data = train_random_forest,
                   method = "ranger",
                   metric = "Accuracy",
                   num.trees = 500)
modelo_rf

plot(modelo_rf)


```


Como podemos ver en la parte de entrenamiento el algoritmo toma distintos valores para las variables aleatorias elegidas en cada corte (mtry) y para el tipo de modelo utilizado para calcular la pureza de cada nodo.

El valor final elegido es mtry = 17 y metodo de Gini con un accuracy de 76.13% y un capa del 32.21% ambos resultados son los mas altos de las pruebas realizadas.





```{r echo=FALSE}
set.seed(342)
pred_prob_rf <- predict(modelo_rf, newdata = test_random_forest)
matriz_confusion_rf = confusionMatrix(data = pred_prob_rf, reference = test_random_forest$CLAIM_FLAG, positive = "SIacc")
matriz_confusion_rf
kable(matriz_confusion_rf$table)%>%
kable_styling()
```
En la prueba vemos que el accuracy es de 77.54% mejorando el train y el kappa de 35.98%. Como otras variables a tener en cuenta para ver la calidad del modelo esta la sensitivity que mide el acierto de los casos positivos entendido como positivo el identificar los que tienen accidente.Por otro lado la specificity nos dice que porcentage de valores negativos acertamos y este caso acertamos un 90% de los valores negativos.

```{r cache=TRUE}

#pred_prob_rf_2 <- predict(modelo_rf, test_random_forest, type = "prob")
#head(pred_prob_rf_2)
#head(modelo_rf)
set.seed(342)
train=sample(1:nrow(train_random_forest),5500)
model.rf=randomForest(train_random_forest$CLAIM_FLAG ~ . , data = train_random_forest, subset = train, importance = TRUE, proximity = TRUE)
model.rf

#head(train.rf)
plot(model.rf)
varImpPlot(model.rf)
```


```{r}
importancia <- as.data.frame(model.rf$importance)

importancia <- rownames_to_column(importancia,var = "variable")


p1 <- ggplot(data = importancia, aes(x = reorder(variable, MeanDecreaseAccuracy),
                               y = MeanDecreaseAccuracy,
                               fill = MeanDecreaseAccuracy)) +
      labs(x = "variable", title = "Reducción de Accuracy") +
      geom_col() +
      coord_flip() +
      theme_bw() +
      theme(legend.position = "bottom")

p2 <- ggplot(data = importancia, aes(x = reorder(variable, MeanDecreaseGini),
                               y = MeanDecreaseGini,
                               fill = MeanDecreaseGini)) +
      labs(x = "variable", title = "Reducción de pureza (Gini)") +
      geom_col() +
      coord_flip() +
      theme_bw() +
      theme(legend.position = "bottom")
ggarrange(p1, p2)
```

### GLM

Para el modelo de regresión lineal en el cual no hay que añadir parametros indicamos a la función que el metodo es un GLM con la metrica de decisión el accuracy y que el modelo es de clasificación.

```{r cache=TRUE}
set.seed(342)
modelo_logistic <- train(CLAIM_FLAG ~ ., data = train_random_forest,
                         method = "glm",
                         metric = "Accuracy",
                         family = "binomial")
modelo_logistic
```

Para este modelo el accuracy es el 77.09% y el kappa es el 33.7%.

```{r echo=FALSE}
pred_prob_glm <- predict(modelo_logistic, newdata = test_random_forest)
confusionMatrix(data = pred_prob_glm, reference = test_random_forest$CLAIM_FLAG, positive = "SIacc")
confusionMatrix(pred_prob_glm, test_random_forest$CLAIM_FLAG)$overall[1]
confusionMatrix(pred_prob_glm, test_random_forest$CLAIM_FLAG)$overall[2]
```

Si nos fijamos el accuracy sube a 77.4% y el kappa a 34.38%. La sensitivity es de 37.4% con una specificity de 92%.

##DECISION TREE

Los árboles de decisión es un modelo de predicción que partiendo de una base de datos crea diagramas de construcciones lógicas que nos ayudan a resolver problemas (Beltrán, 2015).

En este modelo vamos a dejar que train automaticamente haga la poda y nos de los resultados de varias podas.

```{r echo=FALSE, cache=TRUE}
set.seed(342)
modelo_C50Tree <- train(CLAIM_FLAG ~ ., data = train_random_forest,
                    #method = "C5.0Tree",
                    method = "rpart",
                    metric = "Accuracy")
                    
modelo_C50Tree

plot(modelo_C50Tree)
```
El accuracy y el kappa para el complexity parameter de un 0.01 es de 74% y de 17% respectivamente.

```{r, cache=TRUE}
set.seed(342)
modelo.dt <- rpart(CLAIM_FLAG ~ ., method = "class", data = train_random_forest, cp=0.015)
rpart.plot(modelo.dt, extra = 4)
```
```{r}
printcp(modelo.dt)
plotcp(modelo.dt)
```

```{r echo=FALSE}
pred_prob_tree <- predict(modelo_C50Tree, newdata = test_random_forest)
confusionMatrix(data = pred_prob_tree, reference = test_random_forest$CLAIM_FLAG, positive = "SIacc")
```
En este caso vemos que para el test empeora ligeramente pasando a un 73% de accuracy y un 15% de kappa. La sensitivity es de 17% que es realmente baja.

```{r}
test.dt <- predict(modelo.dt, test_random_forest, type = "class")
table(test.dt, test_random_forest$CLAIM_FLAG)

sum(test.dt==test_random_forest$CLAIM_FLAG)/length(test_random_forest$CLAIM_FLAG)
```

###KNN

k-Nearest Neighbour Classification reconoce patrones en los datos, simplemente midiendo la distancia entre grupos de datos. Se trata de uno de los algoritmos más simples y robustos de aprendizaje automático.

```{r echo=FALSE, cache=TRUE}
set.seed(342)
modelo_knn <- train(CLAIM_FLAG ~ ., data = train_random_forest,
                    method = "knn",
                    metric = "accuracy",
                    )
modelo_knn

plot(modelo_knn)
```
vemos que el mejor accuracy lo conseguimos con 9 vecinos  con un 68.8% y un kappa de 10%.

```{r echo=FALSE}
pred_prob_knn <- predict(modelo_knn, newdata = test_random_forest)
confusionMatrix(data = pred_prob_knn, reference = test_random_forest$CLAIM_FLAG, positive = "SIacc")
```
Los resultados del acuracy son de un 71%, un kappa de 10% y una sensitivity de 18%. 

```{r echo=FALSE}
knnPredict <- predict(modelo_knn,newdata = test_random_forest , type="prob")
knnROC <- roc(test_random_forest$CLAIM_FLAG,knnPredict[,"NOacc"])
plot(knnROC, type="S", print.thres= 0.5)
knnROC
```
###SVM

El modelo de maquinas de vectores soporte es un modelo que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo más amplios posibles mediante un hiperplano de separación definido como el vector entre los 2 puntos, de las 2 clases, más cercanos al que se llama vector soporte. Cuando las nuevas muestras se ponen en correspondencia con dicho modelo, en función de los espacios a los que pertenezcan, pueden ser clasificadas a una o la otra clase.

```{r echo=FALSE, cache=TRUE}
set.seed(342)
modelo_svmrad <- train(CLAIM_FLAG ~ ., data = train_random_forest,
                   method = "svmRadial",
                   metric = "Accuracy")
modelo_svmrad

plot(modelo_svmrad)
```
Como vemos el mejor accuracy, que asciende a 76.5% lo obtenemos con el cost parameter mas elevado quiere decir que penalizamos más a los datos dentro de los margenes.
```{r, cache=TRUE}
set.seed(342)
model.svm <- svm(CLAIM_FLAG ~ ., data = train_random_forest, methods = "class",  kernel = "radial")
summary(model.svm)
plot(model.svm, train_random_forest)

```

```{r echo=FALSE}
pred_prob_svm <- predict(modelo_svmrad, newdata = test_random_forest)
confusionMatrix(data = pred_prob_svm, reference = test_random_forest$CLAIM_FLAG, positive = "SIacc")
```
Finalmente en nuestro set de train el Accuracy sube a 77.4% con un 31% de kappa y un 30.8% de sensitivity.

##COMPARATIVA DE MODELOS

Ahora vamos a comparar los distintos modelos con las 25 resamples que se realizan. Para eso vamos a unir todos los resultados y vamos a comparar por la media de los 25.

```{r echo=FALSE}
modelos <- resamples(list(KNN = modelo_knn, logistic = modelo_logistic,
                arbol = modelo_C50Tree, rf = modelo_rf,
                SVMradial = modelo_svmrad))

metricas_resamples <- modelos$values %>%
                         gather(key = "modelo", value = "valor", -Resample) %>%
                         separate(col = "modelo", into = c("modelo", "metrica"),
                                  sep = "~", remove = TRUE)


#metricas_resamples %>% 
#  group_by(modelo, metrica) %>% 
#  summarise(media = mean(valor)) %>%
#  spread(key = metrica, value = media) %>%
#  arrange(desc(Accuracy))

metricas_resamples %>%
  filter(metrica == "Accuracy") %>%
  group_by(modelo) %>% 
  summarise(media = mean(valor)) %>%
  ggplot(aes(x = reorder(modelo, media), y = media, label = round(media, 2))) +
    geom_segment(aes(x = reorder(modelo, media), y = 0,
                     xend = modelo, yend = media),
                     color = "grey50") +
    geom_point(size = 7, color = "firebrick") +
    geom_text(color = "white", size = 2.5) +
    scale_y_continuous(limits = c(0, 1)) +
    # Accuracy basal
    geom_hline(yintercept = 0.62, linetype = "dashed") +
    annotate(geom = "text", y = 0.6, x = 5.5, label = "Accuracy basal") +
    labs(title = "Validación: Accuracy medio repeated-CV",
         subtitle = "Modelos ordenados por media",
         x = "modelo") +
    coord_flip() +
    theme_bw()
```

Como vemos en el gráfico por media el modelo que mejor accuracy obtiene es el GLM junto con el SVM y seguido por el Random forest a pesar que este ultimo fue el que mejor accuracy nos dio en el modulo de test de las ejecuciones anteriores pero pudo ser por realizar un unico lanzamiento.

##REDUCCION DE LA DIMENSIONALIDAD.



###PCA

Principal Component Analysis (PCA) es un método estadístico que permite simplificar la complejidad de espacios muestrales con muchas dimensiones a la vez que conserva su información.

El primer paso que vamos a dar va a ser generar los componentes principales y ver la desviacion estandar, la proporcion de la varianza y la proporcion acumulativa.

Vemos que con 6 componentes principales explicamos el 50% de la varianza


```{r echo=FALSE, cache=TRUE}
pca_train <- prcomp(x = train_kmeans, scale = T, center = T)
summary(pca_train)
names(pca_train)
```
```{r echo=FALSE}
pca_train
```
```{r echo=FALSE}
pca_train$center
```

```{r echo=FALSE}
pca_train$scale
```


```{r echo=FALSE}
biplot(pca_train)
```
```{r echo=FALSE}
#Getting proportion of variance for a scree plot
pr.var <- pca_train$sdev^2
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, 
     xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     ylim = c(0,0.15), 
     type = "b")

pve
```
Aqui podemos ver que el primer componente principal explica un 13% de la varianza y poco a poco va disminuyendo lo que nos da que pensar que no se va a poder hacer una reducción apropiada de la dimensionalidad.


```{r echo=FALSE}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     ylim = c(0, 1), type = "b")
cumsum(pve)
```

Tras el analisis de PCA vemos que para que superemos el 80% de explicabilidad necesitamos 13 componentes y para superar el 90% 15.

###TSNE

Otra de las tecnicas de reducción de la dimensionalidad es stochastic neighbor embedding (tSNE) la cual convierte las distancias euclídeas multidimensionales entre pares de observaciones en probabilidades condicionales.

La ventaja con respecto a PCA es que recoge combinaciones no lineales entre las variables originales.

```{r echo=FALSE, cache=TRUE}
colors = rainbow(length(unique(train$label)))
names(colors) = unique(train$label)

## Executing the algorithm on curated data
tsne <- Rtsne(train_kmeans, dims = 2, perplexity=30, verbose=TRUE, max_iter = 500, check_duplicates = FALSE)
# exeTimeTsne<- system.time(Rtsne(train_kmeans, dims = 2, perplexity=30, verbose=TRUE, max_iter = 500))

resultados <- as.data.frame(tsne$Y)
colnames(resultados) <- c("dim_1", "dim_2")
resultados$numero <- as.character(train_kmeans$CLAIM_FLAG)
ggplot(data = resultados, aes(x = dim_1, y = dim_2)) +
  geom_point(aes(color = numero)) + 
  theme_bw()

```
```{r echo=FALSE}
resultados <- as.data.frame(pca_train$x[, 1:2])
resultados$numero <- as.character(train_kmeans$CLAIM_FLAG)
ggplot(data = resultados, aes(x = PC1, y = PC2)) +
  geom_point(aes(color = numero)) + 
  theme_bw()
```
En este escenario, el método tSNE consigue que, al reducir la dimensionalidad, la separación entre observaciones sea sustancialmente mejor que cuando se emplea PCA.

Véase ahora la reducción a un espacio de 3 dimensiones.

```{r echo=FALSE, cache=TRUE}
library(scatterplot3d)
library(RColorBrewer)
tsne <- Rtsne(X = train_kmeans, is_distance = FALSE, dims = 3, perplexity = 30,
              theta = 0.5, max_iter = 500, check_duplicates = FALSE)

resultados <- as.data.frame(tsne$Y)
colnames(resultados) <- c("dim_1", "dim_2", "dim_3")
resultados$numero <- as.factor(train_kmeans$CLAIM_FLAG)

colores <- brewer.pal(n = 2, name = "Set3")
colores <- colores[as.numeric(resultados$numero)]
scatterplot3d(x = resultados$dim_1,
              y = resultados$dim_2,
              z = resultados$dim_3,
              pch = 20, color = colores, cex.lab = 0.8,
              grid = TRUE, box = FALSE)
legend("bottom", legend = levels(resultados$numero),
      col = colores, pch = 16, 
      inset = -0.23, xpd = TRUE, horiz = TRUE)
```

## HIPERPARAMETROS DE CADA MODELO.

###HIPERPARAMETROS DE GLM

```{r echo=FALSE, cache=TRUE}
# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
particiones  <- 10
repeticiones <- 5

# Hiperparámetros
hiperparametros <- data.frame(parameter = "none")

set.seed(342)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_logistic <- train(CLAIM_FLAG ~ ., data = train_random_forest,
                         method = "glm",
                         tuneGrid = hiperparametros,
                         metric = "Accuracy",
                         trControl = control_train,
                         family = "binomial")
modelo_logistic
```

###HIPERPARAMETROS DE KNN

```{r echo=FALSE, cache=TRUE}
particiones  <- 10
repeticiones <- 5

# Hiperparámetros
hiperparametros <- data.frame(k = c(1, 2, 5, 10, 15, 20, 30, 50, 100, 200))

set.seed(342)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros)) 
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_knn <- train(CLAIM_FLAG ~ ., data = train_random_forest,
                    method = "knn",
                    tuneGrid = hiperparametros,
                    metric = "Accuracy",
                    trControl = control_train)
modelo_knn
```


###HIPERPARAMETROS DE DECISION TREE

```{r echo=FALSE, cache=TRUE}
# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
particiones  <- 10
repeticiones <- 5

# Hiperparámetros
#hiperparametros <- data.frame(parameter = "none")
hiperparametros <- data.frame(cp = c(0.01, 0.011, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017, 0.018, 0.019))
set.seed(342)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_C50Tree <- train(CLAIM_FLAG ~ ., data = train_random_forest,
                    method = "rpart",
                    tuneGrid = hiperparametros,
                    metric = "Accuracy",
                    trControl = control_train)
modelo_C50Tree
```

###HIPERPARAMETROS DE RANDOM FOREST
```{r echo=FALSE, cache=TRUE}
# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
particiones  <- 10
repeticiones <- 5

# Hiperparámetros
hiperparametros <- expand.grid(mtry = c(4),
                               min.node.size = c(1, 2, 3, 4, 5, 10, 15, 20, 30),
                               splitrule = "gini")

set.seed(342)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_rf <- train(CLAIM_FLAG ~ ., data = train_random_forest,
                   method = "ranger",
                   tuneGrid = hiperparametros,
                   metric = "Accuracy",
                   trControl = control_train,
                   # Número de árboles ajustados
                   num.trees = 500)
modelo_rf
```

###HIPERPARAMETROS DE SVM

```{r echo=FALSE, cache=TRUE}
# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
#===============================================================================
particiones  <- 10
repeticiones <- 5


# Hiperparámetros
hiperparametros <- expand.grid(sigma = c(0.001),
                               C = c(200))
#hiperparametros <- expand.grid(sigma = c(0.001),
#                               C = c(1 , 20))

set.seed(342)
seeds <- vector(mode = "list", length = (particiones * repeticiones) + 1)
for (i in 1:(particiones * repeticiones)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(particiones * repeticiones) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control_train <- trainControl(method = "repeatedcv", number = particiones,
                              repeats = repeticiones, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
modelo_svmrad <- train(CLAIM_FLAG ~ ., data = train_random_forest,
                   method = "svmRadial",
                   tuneGrid = hiperparametros,
                   metric = "Accuracy",
                   trControl = control_train)
modelo_svmrad
```

###COMPARATIVA DE MODELOS.

Primero vamos a analizar las medias de las valores que nos ha dado nuestro lanzamiento en train para el resultado que ha sido definitivamente el que mas accuracy tenia en cada modelo.

```{r echo=FALSE, cache=TRUE}
modelos <- list(KNN = modelo_knn, logistic = modelo_logistic,
                arbol = modelo_C50Tree, rf = modelo_rf, SVMRadial = modelo_svmrad)

resultados_resamples <- resamples(modelos)
metricas_resamples <- resultados_resamples$values %>%
                         gather(key = "modelo", value = "valor", -Resample) %>%
                         separate(col = "modelo", into = c("modelo", "metrica"),
                                  sep = "~", remove = TRUE)
metricas_resamples %>% 
  group_by(modelo, metrica) %>% 
  summarise(media = mean(valor)) %>%
  spread(key = metrica, value = media) %>%
  arrange(desc(Accuracy))
```

Como vemos el modelo de Ramdon forest y el logistico están practicamente empatados.

Vamos a verlo de manera gráfica como se distribuyen los valores para cada modelo de las 50 simulaciones realizadas.

```{r echo=FALSE}
metricas_resamples %>% filter(metrica == "Accuracy") %>%
  group_by(modelo) %>% 
  mutate(media = mean(valor)) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(modelo, media), y = valor, color = modelo)) +
    geom_boxplot(alpha = 0.6, outlier.shape = NA) +
    geom_jitter(width = 0.1, alpha = 0.6) +
    scale_y_continuous(limits = c(0, 1)) +
    
    # Accuracy basal
    geom_hline(yintercept = 0.62, linetype = "dashed") +
    annotate(geom = "text", y = 0.6, x = 4.4, label = "Accuracy basal") +
    theme_bw() +
    labs(title = "Validación: Accuracy medio repeated-CV",
         subtitle = "Modelos ordenados por media") +
    coord_flip() +
    theme(legend.position = "none")
```

En principio podemos decir que nos parece con mayor calidad el GLM ya que tiene el diagrama de caja mas estrecho aun teniendo algun outlier.

Para asegurarnos que podriamos diferenciarlos vamos a hacer el test de WILCOXON. Este test es un estadistico que nos ayudará a ver si son significativamente los modelos.


### TEST DE RANGOS DE WILCOXON.

La prueba de los rangos con signo de Wilcoxon es una prueba no paramétrica para comparar el rango medio de dos muestras relacionadas y determinar si existen diferencias entre ellas.


```{r echo=FALSE}
# Comparaciones múltiples con un test suma de rangos de Wilcoxon
# ==============================================================================

metricas_accuracy <- metricas_resamples %>% filter(metrica == "Accuracy")
comparaciones  <- pairwise.wilcox.test(x = metricas_accuracy$valor, 
                                        g = metricas_accuracy$modelo,
                                        paired = TRUE,
                                        p.adjust.method = "holm")

# Se almacenan los p_values en forma de dataframe
comparaciones <- comparaciones$p.value %>%
  as.data.frame() %>%
  rownames_to_column(var = "modeloA") %>%
  gather(key = "modeloB", value = "p_value", -modeloA) %>%
  na.omit() %>%
  arrange(modeloA) 

comparaciones
```

Acorde a las comparaciones por pares, no existen evidencias suficientes para considerar que la capacidad predictiva de los modelos Random Forest, SVM radial y logistica es distinta. 

Por ultimo vamos a comparar todos los modelos

```{r echo=FALSE}
set.seed(342)

pred_prob_glm_final <- predict(modelo_logistic, newdata = test_random_forest)
pred_prob_svm_final <- predict(modelo_svmrad, newdata = test_random_forest)
pred_prob_rf_final <- predict(modelo_rf, newdata = test_random_forest)
pred_prob_knn_final <- predict(modelo_knn, newdata = test_random_forest)
pred_prob_Tree_final <- predict(modelo_C50Tree, newdata = test_random_forest)

paste("El Accuracy de test del modelo GLM:", round(confusionMatrix(pred_prob_glm_final, test_random_forest$CLAIM_FLAG)$overall[1]*100, 3), "%")
paste("El Accuracy de test del modelo SVM:", round(confusionMatrix(pred_prob_svm_final, test_random_forest$CLAIM_FLAG)$overall[1]*100, 3), "%")
paste("El Accuracy de test del modelo rf:", round(confusionMatrix(pred_prob_rf_final, test_random_forest$CLAIM_FLAG)$overall[1]*100, 3), "%")
paste("El Accuracy de test del modelo knn:", round(confusionMatrix(pred_prob_knn_final, test_random_forest$CLAIM_FLAG)$overall[1]*100, 3), "%")
paste("El Accuracy de test del modelo tree:", round(confusionMatrix(pred_prob_Tree_final, test_random_forest$CLAIM_FLAG)$overall[1]*100, 3), "%")

paste("El Kappa de test del modelo GLM:", round(confusionMatrix(pred_prob_glm_final, test_random_forest$CLAIM_FLAG)$overall[2]*100, 3), "%")
paste("El Kappa de test del modelo SVM:", round(confusionMatrix(pred_prob_svm_final, test_random_forest$CLAIM_FLAG)$overall[2]*100, 3), "%")
paste("El Kappa de test del modelo rf:", round(confusionMatrix(pred_prob_rf_final, test_random_forest$CLAIM_FLAG)$overall[2]*100, 3), "%")
paste("El Kappa de test del modelo knn:", round(confusionMatrix(pred_prob_knn_final, test_random_forest$CLAIM_FLAG)$overall[2]*100, 3), "%")
paste("El Kappa de test del modelo tree:", round(confusionMatrix(pred_prob_Tree_final, test_random_forest$CLAIM_FLAG)$overall[2]*100, 3), "%")

```


###Curva ROC

###ROC modelo Random Forest

```{r echo=FALSE}
predicciones_rf_prob_train <- predict(modelo_randomforest, newdata = train_random_forest)
predicciones_rf_prob_test <- predict(modelo_randomforest, newdata = test_random_forest)

pred1 <- prediction(as.numeric(predicciones_rf_prob_test), as.numeric(test_random_forest$CLAIM_FLAG))

perf1 <- performance(pred1, "tpr", "fpr")

perf2 <- performance(pred1, "auc")

perf2

plot(perf1)

```

###ROC modelo SVM

```{r echo=FALSE}
predicciones_rf_prob_train <- predict(modelo_svmrad_final, newdata = train_random_forest)
predicciones_rf_prob_test <- predict(modelo_svmrad_final, newdata = test_random_forest)

pred1 <- prediction(as.numeric(predicciones_rf_prob_test), as.numeric(test_random_forest$CLAIM_FLAG))

perf1 <- performance(pred1, "tpr", "fpr")

perf2 <- performance(pred1, "auc")

perf2

plot(perf1)

```

###ROC modelo GLM

```{r echo=FALSE}
knnPredict <- predict(modelo_logistic_final,newdata = test_random_forest , type="prob")
knnROC <- roc(test_random_forest$CLAIM_FLAG,knnPredict[,"NOacc"])
plot(knnROC, type="S", print.thres= 0.5)
knnROC
```

###ROC modelo KNN Final

```{r echo=FALSE}
knnPredict <- predict(modelo_knn_final,newdata = test_random_forest , type="prob")
knnROC <- roc(test_random_forest$CLAIM_FLAG,knnPredict[,"NOacc"])
plot(knnROC, type="S", print.thres= 0.5)
knnROC
```

###ROC modelo Decision Tree

```{r echo=FALSE}
knnPredict <- predict(modelo_C50Tree_final,newdata = test_random_forest , type="prob")
knnROC <- roc(test_random_forest$CLAIM_FLAG,knnPredict[,"NOacc"])
plot(knnROC, type="S", print.thres= 0.5)
knnROC
```

###ANOVA

La técnica de análisis de varianza (ANOVA) constituye la herramienta básica para el estudio del efecto de uno o más factores (cada uno con dos o más niveles) sobre la media de una variable continua. Es por lo tanto el test estadístico a emplear cuando se desea comparar las medias de dos o más grupos. Esta técnica puede generalizarse también para estudiar los posibles efectos de los factores sobre la varianza de una variable.

```{r echo=FALSE, cache=TRUE}
# =============================================================================

# Se crea una semilla para cada partición y cada repetición: el vector debe
# tener B+1 semillas donde B = particiones * repeticiones.
particiones = 10
repeticiones = 5
set.seed(342)
seeds <- sample.int(1000, particiones * repeticiones + 1)

# Control del filtrado
ctrl_filtrado <- sbfControl(functions = rfSBF, method = "repeatedcv",
                            number = particiones, repeats = repeticiones,
                            seeds = seeds, verbose = FALSE, 
                            saveDetails = TRUE, allowParallel = TRUE)
set.seed(342)
rf_sbf <- sbf(CLAIM_FLAG ~ ., data = train_random_forest,
              sbfControl = ctrl_filtrado,
              # argumentos para el modelo de evaluación
              ntree = 500)
```

```{r echo=FALSE}
rf_sbf
```
```{r echo=FALSE}
rf_sbf$optVariables
```


### CONCLUSIONES

Tras la comparativa realizada con los 5 modelos vemos que tenemos una gran rivalidad entre el modelo GLM, el modelo Random forest y el SVM

La metrica de accuracy y de Kappa para los resultados de train y para los de test nos dan levemente mejor para el modelo de GLM con respecto a los otros dos, además en la gráfica de comparativa de modelos podemos ver que la varianza del modelo es levemente mejor en el GLM aunque tras el analisis de rangos de Wilcoxon vemos que estas diferencias entre los modelos no son significativas por lo que asumimos que en estas métricas están igual.

El punto diferenciador es la curva ROC, este indicador se decanta por el modelo GLM ya que en este modelo tiene un valor de 80% mientras que en el modelo de Random forest y SVM tenemos un valor de 65% y 63% por lo que nos decantamos por el modelo GLM para nuestro estudio.

